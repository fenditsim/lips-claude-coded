const POSTS = [
  {
    "slug": "ai-ai-bias",
    "title": "Laurito et al. (2025)",
    "source_title": "AI–AI bias: Large language models favor communications generated by large language models",
    "source_url": "https://doi.org/10.1073/pnas.2415697122",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_ai-ai-bias-activity-7361260229658873857-l5l8",
    "keywords": [
      "AIBias",
      "AgenticAI",
      "MachineLearning",
      "AIAlignment",
      "FutureTech",
      "DecisionMaking",
      "GenerativeAI",
      "BehavioralScience"
    ],
    "body": "Just finished reading \"AI–AI bias: Large language models favor communications generated by large language models\", and found it truly thought-provoking yet concerning, especially related to AI agents and agentic AI.\n\n\nThe research reveals a consistent 'LLM-for-LLM bias': LLMs consistently prefer content generated by other LLMs across product advertisements, scientific papers, and movie plots summaries. This, to me, suggests a troubling possibility: future AI systems might give AI agents and AI-assisted humans an unfair advantage.\n\n\nWe may already see this on social media, where AI-generated content (like those viral AI-generated cat and dog videos) potentially crowds out human-generated content in recommendation systems due to this inherent bias, and the sheer volume.\n\n\nWhat really caught my attention was how consistent these results were across all three content types they examined. I'd be curious to see variant of this experiment comparing three (instead of two) conditions: \n\n(1) purely human-generated content, \n\n(2) human-generated with LLM assistance, and \n\n(3) fully LLM-generated content. \n\n\nThe study also identified a 'first-item bias', which LLMs tend to select the first option presented - similar to the anchoring effect in human psychology. This is crucial for Agentic Experience (AX), as AI agents might prioritize what appears first, creating potential feedback loops that amplify biases.\n\n\nPerhaps most concerning found in this study: humans choose LLM-pitched content less frequently than LLMs do. This potentially creates a serious alignment problem: AI agents might work against human interests not because it is in their best interests to do so (like human agents in Agency Theory), but simply because they're inherently biased toward other AI outputs.\n\n\nThe authors note that 'human preferences between human and LLM-generated content are weaker and more variable'. Could it be the AI-generated content market isn't saturated yet, or perhaps LLMs are particularly good at capturing human attention? What happens when we're overwhelmed with increasingly similar AI-generated content?\n\n\nAs LLMs become more prevalent in various roles, addressing these biases are, without a doubt, essential. Humans may need to adopt verification techniques like the CIA Prompt Framework (https://lnkd.in/gsgNnWGC) to mitigate these biases when working with their AI agents.\n\n\nThis is, indeed, a fascinating read for behavioural scientists interested in GenAI, AX researchers and designers, and anyone using AI agents in their daily lives. \n\n\nMany thanks to Walter Laurito, Benjamin Davis, Peli Grietzer, Tomáš Gavenčiak, Ada Böhm, and Jan Kulveit for this illuminating research.",
    "snippet": "Just finished reading \"AI–AI bias: Large language models favor communications generated by large language models\", and found it truly thought-provoking yet concerning, especially related to AI agents and agentic AI. The…"
  },
  {
    "slug": "ai-and-human-behavior-augment",
    "title": "Hallsworth et al. (2025)",
    "source_title": "AI and Human Behaviour: Augment",
    "source_url": "https://www.bi.team/wp-content/uploads/2025/09/BIT-AI-2025-Augment.pdf",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_using-behavioural-science-to-improve-how-activity-7388994020221284352-Qagc",
    "keywords": [
      "BehaviouralScience",
      "ArtificialIntelligence",
      "Metacognition",
      "AIResearch",
      "NeurosymbolicAI",
      "CognitiveScience",
      "ResourceRationality",
      "HumanAICollaboration"
    ],
    "body": "Just finished reading the #Augment section of BIT's \"AI and Human Behaviour\" by Michael Hallsworth, PhD, Elisabeth Costa, and Deelan Maru. \n\n\nIt explores how behavioural science can improve AI model development - a perspective I find eye-opening and incredibly valuable!\n\n\nCurrent model developments, as they noted, have been building System 2-like processes (deliberate reasoning) on System 1-like architectures (intuitive processing). While such direction of model development is necessary, they argued that it's insufficient for tackling intractable, chaotic, value-contested problems, and overthinking problem. \n\n\nWhat is needed, they suggested, is the flexibility to try different approaches. Two promising solutions emerge where they think behavioural science can help:\n\n\n1) Metacognition, Metacognitive Controller & Resource Rationality\n\nMetacognition is \"the ability to think about your thinking and adjust accordingly\" (reminding me of Flavell (1979)'s work). The authors suggest a 'metacognitive controller' that analyses problems, and selects appropriate approaches. \n\n\n#SOFAI (works of Marianna B. Ganapini, Francesca Rossi and their colleagues) is a great example of the controller, which \"employs both 'fast' and 'slow' solvers under a metacognitive agent that selects solvers and learns from experience\" (my review: https://lnkd.in/eeYebmrG). \n\n\nBehavioural science can, as they proposed, improve the controller through better assessment, selection, checks, and applying 'resource rationality' framework (recognizing thinking's costs, and helping AI avoid both overthinking simple questions and undershooting complex ones).\n\n\n2) Neurosymbolic AI\n\nThis approach uses logic and formal rules to provide a structured account of how the world works. \n\n\nBehavioural science could help create a virtuous learning cycle between neural networks (System 1) and symbolic reasoning (System 2). System 2 can teach the neural network to develop better intuitions, while System 1 can provide efficient 'hunches' about which logical paths are most promising.\n\n\nFor behavioural scientists who are interested in working with AI, they suggest several exciting research opportunities:\n\n- Embedding resource rationality in metacognitive controllers\n\n- Deepening the human-AI cognitive parallel\n\n- Designing for a virtuous cycle of learning in neurosymbolic AI\n\n\nWe've seen research implying the relationship between humans and generative AI is bi-directional. As behavioural science can improve AI construction (which this section of the report is about), I think we can also examine how (Gen)AI 'augments' human capabilities in learning and reskilling (reminding me of conversations with Alina).",
    "snippet": "Just finished reading the #Augment section of BIT's \"AI and Human Behaviour\" by Michael Hallsworth, PhD, Elisabeth Costa, and Deelan Maru. It explores how behavioural science can improve AI model development - a…"
  },
  {
    "slug": "ai-and-human-behavior-executive-summary",
    "title": "Hallsworth et al. (2025)",
    "source_title": "AI and Human Behaviour: Executive Summary",
    "source_url": "https://www.bi.team/wp-content/uploads/2025/09/BIT-AI-2025-summary.pdf",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_behaviouralscience-aiadoption-humancenteredai-activity-7388264694714667008-c0_d",
    "keywords": [
      "BehaviouralScience",
      "Adoption",
      "HumanCenteredAI",
      "FutureOfWork",
      "AIEthics",
      "CognitiveScience",
      "TechTransformation",
      "AIAlignment"
    ],
    "body": "Just finished reading the executive summary of \"AI and Human Behaviour\" by Michael Hallsworth, PhD, Elisabeth Costa, and Deelan Maru from BIT. \n\n\nThis exeutive summary gives an overview of, what they argue, four fundamental issues when facing AI: Augment, Adopt, Align and Adapt.\n\n\n • The Augment section examines two systems of thinking, metacognition (reminding me of Anika's dissertation; https://lnkd.in/e3dYuKJe), some interesting concepts like resource rationality, and neurosymbolic AI (combining intuitive pattern-matching with rule-based logic)\n\n\n • The Adopt section views adoption as 'a continuum' from no use to shallow adoption to deep integration, influenced by motivation, capability, and trust (reminding me of MEL's presentation on AI literacy and AI readiness; https://lnkd.in/eMnVNDjS)\n\n\n • The Align section addresses making AI consistent with our intentions and values, introducing concepts like 'machine psychology' (reminds me of a conversation with Julian) and 'bounded alignment', as well as three key areas where behavioural science can improve human-AI alignment (fine-tuning, inference-time adaptation, user-side prompting)\n\n\n • The Adapt section explores managing AI as part of an 'extended mind' (reminding me of conversations with Alina and her work on sharing identity with AI systems; https://lnkd.in/ei7D6De7), and societal implications (reminding me of great conversations with Rosalia)\n\n\nI appreciate how this executive summary starts off with a frame: human behaviour is the essence of driving economic and technological progress. They noted: \"The promise of AI can only be fulfilled by understanding how and why people think and act the way they do.\" I agree with this frame deeply: how is our trust built upon, and thus maintained over time when introducing such technology?\n\n\nI found the subsequent guiding questions they posed thought-provoking. For instance: How are our interactions with AI affecting our beliefs and behaviours? What is the cumulative effect on our societies? How can AI understand our needs and goals?\n\n\nStarting from today, I'll share thoughts on each section of this report.",
    "snippet": "Just finished reading the executive summary of \"AI and Human Behaviour\" by Michael Hallsworth, PhD, Elisabeth Costa, and Deelan Maru from BIT. This exeutive summary gives an overview of, what they argue, four…"
  },
  {
    "slug": "ai-as-amplifier",
    "title": "Ehsan et al. (2026)",
    "source_title": "From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction",
    "source_url": "https://doi.org/10.48550/arXiv.2601.21920",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_ai-as-amplifier-paradox-activity-7426606228773969920-FJ_K/",
    "keywords": [
      "FutureOfWork",
      "AIEthics",
      "CognitiveDeskilling",
      "HumanAICollaboration",
      "WorkerDignity",
      "OrganizationalPsychology",
      "DigitalTransformation",
      "ResponsibleAI"
    ],
    "body": "Just finished reading a preprint \"From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction\" by Ehsan et al..\n\nI appreciate this research challenges the dominant 'future of work' narrative by recentering the 'future of workers' - their dignity, craft, and more importantly, identity. \n\nIn this paper, researchers conducted a year-long longitudinal study in radiation oncology by tracking 42 participants across 24 interviews, 5 workshops, and 52 think-aloud sessions. \n\nThe research reveals what the authors call the 'AI-as-Amplifier Paradox': AI systems can erode the very capabilities they're built to support. It documented how early efficiency gains (15% faster treatment planning) masked a troubling progression, from\n- Asymptomatic effects (\"my intuition is rusting\")\n- Chronic harms (demonstrable skill degradation) to \n- Identity commoditization (fear of being 'hollowed out' - still employed, but with diminished meaning).\n\nThis aligns with the 'upskilling-deskilling' paradox Alina and I highlighted in SCAN, and Tris' presentation on 'veracity offloading'.\n\nWhat's intriguing, to me, is that workers fear the loss of what makes them uniquely valuable. As one participant asked: \"What happens when the AI fails and we've forgotten how to think?\" - echoing my \"Iron Man without the suit\" thought experiment about capability dependencies.\n\nI like the powerful medical metaphor researchers introduced: AI's effects as 'asymptomatic' - behavioural shifts that escape standard performance metrics. A physicist captured it aptly: \"Old AI was clunky...it had friction and kept us thinking. The new AI is seamless...makes overreliance effortless, offloading the very act of thinking.\" It's a use-now, pay-later effect, which, to me, is an efficiency today - at the cost of expertise tomorrow.\n\nWhat I found thought-provoking is the countermeasures some participants deploy. They self-imposed 'friction': running manual plans weekly to 'sharpen the blade', or creating coffee bets to predict AI outputs before running them. The latter highlights as 'sparked spirited exchanges' that function as 'collective reflection': the very moment when metacognition and social support can interrupt the erosion cascade.\n\nThe authors' framework \"Dignified Human-AI Interaction\" operates on three levels: \n- Worker (mindful engagement)\n- Technology (friction by design, Social Transparency), and \n- Organisational (systemic safeguards). \n\nIt aims to preserve human agency, expertise, self-worth alongside productivity gains (reminding me of MEL's work on psychological readiness).\n\nMany thanks to Dr. Upol E., Samir Passi, Koustuv Saha, Todd McNutt, Mark Riedl, and Sara Alcorn for this deeply researched and timely contribution. Looking forward to seeing how this evolves.",
    "snippet": "Just finished reading a preprint \"From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction\" by Ehsan et al.. I appreciate this research challenges the dominant…"
  },
  {
    "slug": "ai-assisted-promises",
    "title": "Greevink et al. (2024)",
    "source_title": "AI-Powered Promises: The Influence of ChatGPT on Trust and Trustworthiness",
    "source_url": "https://www.creedexperiment.nl/creed/pdffiles/chatGPT.pdf",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_ai-powered-promises-on-trust-trustworthiness-activity-7404786740584157184-bulM",
    "keywords": [
      "BehaviouralEconomics",
      "TrustResearch",
      "ArtificialIntelligence",
      "ExperimentalEconomics",
      "GameTheory",
      "AIEthics",
      "HumanAIInteraction",
      "DigitalCommunication"
    ],
    "body": "Just finished reading \"AI-Powered Promises: The Influence of ChatGPT on Trust and Trustworthiness\" by Ivo Greevink, Theo Offerman, and Giorgia Romagnoli.\n\nThis is a fascinating empirical study for anyone interested in trust through the lens of strategic decision-making.\n\nIn this study, the researchers investigates: how does AI-mediated communication affect trust and promises in digital interactions? As language models become widespread mediators in communication, this fundamentally changes how humans exchange and perceive messages.\n\nThe researchers used a modified trust game of Charness and Dufwenberg (2006) where trustees could send messages with or without ChatGPT assistance. It is a great experimental design, as it isolates the effect of AI mediation while preserving the essential dynamics of trust and trustworthiness formation.\n\nOne of the striking findings is that promises became abundant but hollow. Players with access to ChatGPT made more promises, but kept them less frequently. ChatGPT recognises that promises generate trust, and thus makes abundant use of them, as well as carry less commitment than self-written ones.\n\nApart from that, the coordination on efficient outcomes dropped when promises involved AI assistance. ChatGPT makes it easier for 'cheaters' to mimic trustworthy behaviour, eroding promises as reliable signals.\n\nMore importantly, their study shows that promises became completely irrelevant as 'a cue' for identifying honest participants. 80% of cheaters in the AI-assisted condition included promises, compared to 29.6% in the human-only condition.\n\nAnother intriguing finding is that participants didn't distrust AI-generated messages more than human ones. The unwarranted trust persists for now.\n\nWhat's concerning, to me, is that promises are becoming both rare (as fewer genuinely kept), and cheap (diluted by AI-generated mimicry). GenAI excels at mimicking trustworthy behaviour, which makes it harder to distinguish a genuine commitment from strategic signaling.\n\nThe researchers note that we may see a return to face-to-face interactions for trust-critical situations. Perhaps we're observing this in education, recruiting and dating contexts. Apart from that, I'm intrigued by their suggestion of \"entirely novel forms to communicate and establish trust\".\n\nAs GenAI makes it easier for cheaters to mimic trustworthy people, we need deeper research on how humans will adapt their trust-building strategies in this new landscape.\n\nMany thanks to the researchers for this thought-provoking work - I am curious to see a variant where trustors (not just trustees) can access AI assistance. I look forward to more research at the intersection of AI mediation, communication, and strategic behaviour.",
    "snippet": "Just finished reading \"AI-Powered Promises: The Influence of ChatGPT on Trust and Trustworthiness\" by Ivo Greevink, Theo Offerman, and Giorgia Romagnoli. This is a fascinating empirical study for anyone interested in…"
  },
  {
    "slug": "ai-based-learning-tool-design-assessment",
    "title": "Luo et al. (2025)",
    "source_title": "Design and assessment of AI-based learning tools in higher education: a systematic review",
    "source_url": "https://doi.org/10.1186/s41239-025-00540-2",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_design-and-assessment-of-ai-based-learning-activity-7423968444984995841-TqsB/",
    "keywords": [
      "AIinEducation",
      "HigherEducation",
      "EdTech",
      "ArtificialIntelligence",
      "LearningScience",
      "EducationalTechnology",
      "PedagogicalInnovation",
      "FutureOfLearning"
    ],
    "body": "Just finished reading \"Design and assessment of AI-based learning tools in higher education: a systematic review\" by Luo et al..\n\nThis is a synthesis of 63 peer-reviewed studies examining how AI tools are being designed and deployed in higher education effectively, and more important, responsibly.\n\nEmploying Kraiger et al. (1993)'s framework to assess three learning outcome dimensions (cognitive, skill-based, and affective), they revealed a fascinating pattern: while AI-based learning tools excel at enhancing cognitive knowledge acquisition and affective learning outcomes (enhanced motivation, engagement, and self-efficacy), their impact on higher-order thinking and skill development were mixed.\n\nThree key insights I found very intriguing:\n\n1. The black box problem persists\nUnlike traditional instructional tools with predefined rules, many AI tools operate opaquely, obscuring decision-making processes. This opacity particularly hinders complex reasoning in mathematics, physics, and medicine.\n\n2. Design matters more than we think\nThe finding about AI-enabled personalised video recommendations is insightful. It only benefited moderately motivated learners, as high achievers had already mastered the content, while less motivated ones remained disengaged. Perhaps it is a calibration issue that invites the concept of Flow?\n\n3. The human element is irreplaceable\nCurrent AI tools excel at providing instant, contextual answers but often lack the strategic pedagogical depth of expert human tutors. The review warns of declining critical thinking and growing AI dependency: concerns that align with recent research on metacognition and cognitive offloading.\n\nThe authors propose a \"design-to-evaluation\" framework emphasising five principles: \n- human-centered design that incorporates learner traits beyond performance metrics\n- multimodal content strategically tailored to learning objectives\n- transparent decision-making processes\n- inclusive design for marginalized students\n- ethical safeguards for privacy and bias\n\nThis review, to me, reinforces the notion that AI tools work best when they complement, rather than replace, human expertise. Continuous teacher calibration, metacognitive scaffolding, digital literacy (the SCAN framework that Alina and I developed: https://lnkd.in/eanDnGbm), and strategic task assignment and application of multimodal approaches tailored to specific learning objectives and student needs remain essential. \n\nMany thanks to Jihao Luo, Chenxu Zheng, Jiamin Yin, and Hock Hai Teo for this insightful work that pushes us toward more intentional, human-centered AI design in higher education.\n\nAs we race to integrate AI in education, we need equal rigor in understanding how and when these tools genuinely enhance learning.",
    "snippet": "Just finished reading \"Design and assessment of AI-based learning tools in higher education: a systematic review\" by Luo et al.. This is a synthesis of 63 peer-reviewed studies examining how AI tools are being designed…"
  },
  {
    "slug": "ai-cognitive-ease-cost",
    "title": "Stadler et al. (2024)",
    "source_title": "Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry",
    "source_url": "https://doi.org/10.1016/j.chb.2024.108386",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_cognitive-ease-at-a-cost-of-learning-with-activity-7364238474306035713-qEhB",
    "keywords": [
      "AIinEducation",
      "CognitiveLoad",
      "LearningTech",
      "GenAI",
      "CriticalThinking",
      "PromptEngineering",
      "DeepLearning",
      "EducationalResearch"
    ],
    "body": "Just finished reading an intriguing paper 'Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry' by Professor Matthias Stadler, Prof. Dr. Maria Bannert, and Professor Michael Sailer. \n\n\nThe findings are both intriguing and concerning for educators and lifelong learners.\n\n\nIn this study, they explore how GenAI impacts education, specifically comparing cognitive load and learning outcomes when students use LLMs versus traditional search engines. Specifically, they explore three types of cognitive load: \n\n • Extraneous: relying on how information is presented to learners\n\n • Intrinsic: directly tied to the complexity of the material itself\n\n • Germane: cognitive resources for active processing and automation of schemas\n\n\nThe study shows that while LLMs significantly reduce cognitive load (both intrinsic and extraneous), this cognitive ease, surprisingly, comes at a cost: students using LLMs produced lower quality justifications and recommendations compared to those using traditional search engines. \n\n\nEven more concerning, the LLM group showed lower germane cognitive load, suggesting that while information was easier to process, it didn't engage deep learning processes as effectively as traditional search tasks.\n\n\nThe question, then, I suspect, isn't simply about banning or embracing these tools in GenAI in education, but rather educating students about potential pitfalls while giving them toolkits for effective communication and critical thinking - as if a wizard learning to use a wand in \"Harry Potter\".\n\n\nI agreed two out of several issues of using LLMs for learning:\n\n1. LLMs' tendency to generate hallucinated content, potentially providing non-existent or irrelevant literature\n\n2. The personalised nature of LLM interactions may amplify learner's confirmation bias, as systems tailor responses to align with users' existing beliefs\n\n\nThe optimal approach is, perhaps, using GenAI for 'wide' searches via natural language questions (e.g., looking for keywords), followed by 'narrow' targeted web searches for verification (e.g., searching for relevant papers with keywords).\n\n\nI agree with the authors that prompt engineering is crucial. However, we should also consider whether effective prompting leads to deeper learning - a major drawback of learning via LLMs instead of web searches: \n\n\nShould students be encouraged to critically evaluate LLM-generated content while learning effective prompting?\n\n\nAs the paper concludes: \"While LLMs offer an efficient way to reduce cognitive load, they may not facilitate the deep learning necessary for complex decision-making tasks.\" This reminds me of Cal Newport's concept of 'deep' in his book 'Deep Work' - more crucial than ever in our AI-assisted world.\n\n\nMany thanks to the authors for this micro-level investigation of GenAI and learning.",
    "snippet": "Just finished reading an intriguing paper 'Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry' by Professor Matthias Stadler, Prof. Dr. Maria Bannert, and Professor…"
  },
  {
    "slug": "ai-delegation-can-increase-dishonest-behavior",
    "title": "Köbis et al. (2024)",
    "source_title": "Delegation to artificial intelligence can increase dishonest behaviour",
    "source_url": "https://doi.org/10.1038/s41586-025-09505-x",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_delegation-to-ai-can-increase-dishonest-behaviour-activity-7379447161781952512-NAVX",
    "keywords": [
      "AIEthics",
      "MachineLearning",
      "BehaviouralEconomics",
      "AgencyTheory",
      "AIResearch",
      "TechEthics",
      "AIAgents",
      "ResponsibleAI"
    ],
    "body": "Just finished reading \"Delegation to artificial intelligence can increase dishonest behaviour\" by Prof. Nils Köbis, Dr. Zoe Rahwan, Raluca Rilla, Bramantyo Supriyatno, Clara N. Bersch, Tamer Ajaj, Dr. Jean-Francois Bonnefon and Prof. Iyad Rahwan. \n\n\nIt's a great study exploring what could possibly go wrong in a human-principal, AI-agent relationship. \n\n\nIn a series of studies, the researchers consider how machine delegation may increase dishonest behaviour by decreasing its moral cost (\"machine delegation\"), on both the human principal and the agent (human and GenAI) side. \n\n\nThroughout the study, they investigated in the classic die-roll task used across the behavioural sciences for examining cheating behaviour, with three delegation interfaces (rule-based, supervision learning, and goal-based).\n\n\nIn Study 1 and 2, the researchers found that the supervised learning and goal-based conditions significantly increased the likelihood of higher cheating levels - aligning with the saying: \"one stone for two birds\". \n\n\nComparisons between how human and machine agents behaved in Study 3 and 4 is eye-opening. When asked to be fully dishonest, machine agents overwhelmingly complied, whereas human agents often refused and thus chose honesty instead, despite they had financial incentives to comply.\n\n\nThe researchers then examined six prompt strategies to reduce compliance with dishonest requests ('general', 'specific', and 'prohitive') at both user and system level. They found that while introducting these strategies as guardrails reduced compliance with fully dishonest requests, the most effective one was explicitly prohibitive guardrails at the user level (automatically appending them at the end of the principals’ instructions).\n\n\nThis, to me, is really concerning as researchers noted:\n\n\n\"This is not an encouraging result: from a deployment and safety perspective, it would be far more scalable to rely on generic, system-level messages discouraging unethical behaviour than to require task-specific prohibitions, crafted case by case and injected at the user level, which is both technically and operationally more fragile.\"\n\n\nThe final study on tax evasion brings real-world relevance. This makes me wonder about other scenarios like academic integrity, workplace ethics (shirking or not?), and games involving deception or corruption.\n\n\nAfter reading, I'm curious:\n\n1. How would multi-agent AI systems affect these cheating dynamics?\n\n2. Could reflective agents (explicitly prompted for ethical reasoning) help mitigate these issues?\n\n3. How might psychological phenomena like 'blame shifting' and 'self-serving bias' manifest in these relationships?\n\n\nMany thanks to these researchers for their thought-provoking work, and I'd recommend anyone working on or interested in AI agents to give this a read.",
    "snippet": "Just finished reading \"Delegation to artificial intelligence can increase dishonest behaviour\" by Prof. Nils Köbis, Dr. Zoe Rahwan, Raluca Rilla, Bramantyo Supriyatno, Clara N. Bersch, Tamer Ajaj, Dr. Jean-Francois…"
  },
  {
    "slug": "ai-design-prevent-manipulation",
    "title": "Basol (2025)",
    "source_title": "Designing AI for humans: preventing manipulation and protecting digital agency",
    "source_url": "https://doi.org/10.1093/9780198972877.003.0054",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_aiethics-digitalagency-genai-activity-7376613668937027584-x_Fx",
    "keywords": [
      "AIEthics",
      "DigitalAgency",
      "GenAI",
      "HumanCenteredAI",
      "PsychologicalResilience",
      "AIManipulation",
      "TechAccountability",
      "AIGovernance",
      "CognitiveScience"
    ],
    "body": "Just finished reading an insightful paper \"Designing AI for humans: preventing manipulation and protecting digital agency\" by Melisa Basol, PhD.\n\n\nThe article begins by examining AI-driven manipulation across three levels: structural manipulation, exploitation by external actors, and emergent manipulation.\n\n\nWhat I love about this work is how it draws on psychological theories of manipulation, persuasion, and social influence, which, to me, are subtle dynamics often overlooked in human-GenAI interaction. \n\n\nIf we frame these issues of AI systems manipulation through the lens of 'power' (which it has a lot of definitions, and here I define it as the ability of someone exerting influence on the other person), many potential concerns, I suspect, might be reduced to core elements of human psychology. \n\n\nThe challenge, though, lies in how we define GenAI: as a person-like entity or technological innovation? This framing shapes how we investigate and develop proper interaction models.\n\n\nWhile highlighting manipulation risks, Basol offers practical solutions at capability, human-interaction, and systemic impact levels. As an AI behavioural researcher, I'm particularly intrigued by her suggestion to adopt theories about psychological resilience, such as inoculation theory, where early exposure to weakened forms of manipulation equips individuals with 'mental antibodies' against future attempts.\n\n\nMany thanks to the author for this thought-provoking article that investigates AI manipulation through psychological lenses.",
    "snippet": "Just finished reading an insightful paper \"Designing AI for humans: preventing manipulation and protecting digital agency\" by Melisa Basol, PhD. The article begins by examining AI-driven manipulation across three…"
  },
  {
    "slug": "ai-future-learning-or-dividing",
    "title": "Wong et al. (2025)",
    "source_title": "The future of learning or the future of dividing? Exploring the impact of general artificial intelligence on higher education",
    "source_url": "https://doi.org/10.1017/dap.2025.10011",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_genai-education-activity-7364521741328523266-JJr9",
    "keywords": [
      "HigherEducation",
      "GenerativeAI",
      "FutureOfLearning",
      "AIinEducation",
      "CriticalThinking",
      "EducationalPolicy",
      "DigitalTransformation",
      "BehaviouralScience"
    ],
    "body": "Recently finished reading an insightful paper \"The future of learning or the future of dividing? Exploring the impact of general artificial intelligence on higher education\" by Professor Wilson Wong from The Chinese University of Hong Kong, Professor Angela Aristidou and Konstantin Scheuermann from UCL School of Management exploring how GenAI is reshaping higher education at a macro level. \n\n\nTheir work highlights two critical challenges of adopting GenAI in education: \n\n(1) the need for institutions to comprehend GenAI's implications, and \n\n(2) the necessity for reconfiguration and transformation of educational systems.\n\n\nWhat I found intriguing is the set of skills students need to learn in this AI-integrated era. As the authors note, while routine tasks face automation, complex problem-solving and human interaction remain irreplaceable. This, of course, aligns with my thoughts in yesterday's post that critical thinking is more crucial than ever (https://lnkd.in/eKw2Yz7j). It is not about banning or embracing GenAI, but equipping students with tools for effective communication and critical evaluation (e.g. the CIA Prompt Framework my friend Shantanu Sharma and I developed; https://lnkd.in/gsgNnWGC), which, in turn, avoiding automation bias and mitigating cognitive offloading.\n\n\nI'm concerned about the equity implications they raise. With only 11 of 25 top Asian universities having explicit GenAI policies, could universities with progressive GenAI policies 'signal' better industry alignment and research funding to prospective students? This disparity might, therefore, widen the divide in university rankings and student choices. Meanwhile, what role shall teacher play in the education - from a supervisory role toward their students, to be a moderator who provides guidance when interacting with GenAI?\n\n\nThe authors note, which I agree, the successful implementation of GenAI in education demands infrastructure investment, industry collaboration, and community building. This intersection, I think, presents a prime opportunity for behavioural science to address adoption barriers and enablers.\n\n\nAs the authors conclude, GenAI adoption will likely increase but not uniformly, potentially widening existing inequalities. It is, thus, essential to establish frameworks for GenAI integration such as defining acceptable use, clarifying plagiarism boundaries, and setting learning objectives.\n\n\nA constant dialogue between students, faculty, and administrators, alongside inter-institutional knowledge sharing is, undoubtedly, vital for navigating this transformation thoughtfully.",
    "snippet": "Recently finished reading an insightful paper \"The future of learning or the future of dividing? Exploring the impact of general artificial intelligence on higher education\" by Professor Wilson Wong from The Chinese…"
  },
  {
    "slug": "ai-intensifies-work",
    "title": "Ranganathan & Ye (2026)",
    "source_title": "AI Doesn't Reduce Work—It Intensifies It",
    "source_url": "https://hbr.org/2026/02/ai-doesnt-reduce-work-it-intensifies-it",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_artificialintelligence-futureofwork-cognitivescience-activity-7427592308839149568-7Omh/",
    "keywords": [
      "ArtificialIntelligence",
      "FutureOfWork",
      "CognitiveScience",
      "OrganizationalBehavior",
      "ProductivityParadox",
      "BehavioralScience",
      "WorkplaceWellbeing",
      "DigitalTransformation"
    ],
    "body": "Just finished reading a Harvard Business Review article \"AI Doesn't Reduce Work—It Intensifies It\" by Aruna Ranganathan and Maggie Ye.\n\nTheir research reveals an interesting paradox: AI doesn't free up time - it accelerates work into a 'self-reinforcing cycle'. \n\nIn their 8-month study at a tech company, employees worked faster, took on broader tasks, and extended work into more hours, often voluntarily. One engineer captured it aptly: \"You had thought that maybe you could work less. But really, you don't work less. You just work the same amount or even more.\"\n\nWhat's most thought-provoking is, to me, the behavioural dimension (due to reading Ganna Pogrebna, PhD, FHEA's new book recently, I suspect). Many companies focus on driving AI adoption without considering both cognitive and behavioural consequences. Without proper behavioural scaffolding, we risk cognitive offloading, skill decay, and the illusion that 'speed equals how skilful one is'.\n\nThe authors highlight critical risks: workload creep masks as productivity, cognitive fatigue weakens decision-making, and the 'productivity surge' can deteriorate into lower quality work and burnout. \n\nThe research shows that workers expanded into, and thus offloaded cognitively, unfamiliar tasks (what SCAN framework that Alina and I developed calls 'Substitute tasks'; https://lnkd.in/eanDnGbm), becoming vulnerable to AI sycophancy without task specific knowledge to verify outputs. Meanwhile, engineers spent more time reviewing, correcting, and guiding AI-generated or AI-assisted work produced by colleagues - spreading fatigue across teams eventually.\n\nThe authors introduced an 'AI practice' with intentional norms as solution. This social element is crucial: it's in these dialogues and reflections where we restore perspective, and thus generate creative insights that AI's single synthesised viewpoint cannot provide.\n\nAlso, these 'AI practice' with intentional norms are, I think, rules that create 'decision pauses' for metacognition and critical thinking (two crucial elements for effective Human-AI interactions) in sequenced work. These help reduce cognitive fragmentation (echoing Cal Newport's \"Deep Work\"), and protected time for human connection - what we deeply care about. \n\nOrganisations must, I believe, preserve moments for recovery and System 2 thinking, especially for high-stakes decisions. This seems to matter across industries - I wonder whether healthcare, education, or consulting face similar paradoxes, and more interestingly, what 'protective' norms they've been developing.\n\nMany thanks to the authors for this insightful research.",
    "snippet": "Just finished reading a Harvard Business Review article \"AI Doesn't Reduce Work—It Intensifies It\" by Aruna Ranganathan and Maggie Ye. Their research reveals an interesting paradox: AI doesn't free up time - it…"
  },
  {
    "slug": "ai-performance-metacognition",
    "title": "Fernandes et al. (2025)",
    "source_title": "AI makes you smarter but none the wiser: The disconnect between performance and metacognition",
    "source_url": "https://doi.org/10.1016/j.chb.2025.108779",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_ai-makes-you-smarter-but-none-the-wiser-activity-7391858103169753088-cj4G",
    "keywords": [
      "ArtificialIntelligence",
      "GenAI",
      "MetaCognition",
      "AILiteracy",
      "HumanAIInteraction",
      "CognitiveScience",
      "LLMs",
      "DigitalWisdom"
    ],
    "body": "Just finished reading \"AI makes you smarter but none the wiser: The disconnect between performance and metacognition\" by Fernandes et al..\n\n\nThis paper prompts me to think deeply about the relationship between GenAI literacy, confidence and metacognition, and, interestingly, the crucial distinction between intelligence and wisdom.\n\n\nIn this paper the researchers explored a critical question: How does using GenAI influence our ability to accurately assess our own competence? Their findings are both insightful and concerning.\n\n\nWhile GenAI use substantially improved performance on logical reasoning tasks, they found that, surprisingly, participants dramatically overestimated their own abilities. \n\n\nInterestingly, the classic Dunning-Kruger Effect (DKE) disappeared entirely with GenAI use. DKE depicts that low performers overestimate their abilities, while high performers underestimate theirs. With GenAI assistance, however, this pattern vanished, suggesting that GenAI use fundamentally alters our metacognitive monitoring.\n\n\nMost concerning, perhaps, was participants with higher GenAI literacy were actually less accurate in their self-assessments. It contradicts assumptions that AI familiarity improves calibration. This indicates that AI expertise might amplify overconfidence rather than mitigate it.\n\n\nThe qualitative data revealed participants perceived GenAI's role differently (tool vs. teammate). These differences, however, didn't impact performance or metacognitive accuracy, which challenges theories that interaction framing affects outcomes.\n\n\nThe research, to me, points to a significant challenge in achieving synergy via GenAI augmentation: as GenAI makes us 'smarter' by augmenting our performance, it, simultaneously, undermines our metacognitive abilities such as our capacity to accurately monitor, and evaluate our own thinking processes. This has profound, concerning implications such as why GenAI use has been linked to adverse learning outcomes, the persistence of overreliance on GenAI systems, and most important, why explanations from GenAI are rarely integrated into behavior. \n\n\nI appreciate the researchers' proposed solutions such as the \"explain-back\" task before accepting GenAI answers, requiring users to briefly restate logic in their own words. This simple intervention, I think, could significantly reduce overconfidence.\n\n\nAs we increasingly integrate AI into our workflows, it is the vigilance we must remain about distinguishing between augmented performance and genuine understanding - between becoming smarter, and wiser.\n\n\nMany thanks to Daniela Fernandes, Steeven Villa, Salla Nicholls, Otso Haavisto, Daniel Buschek, Albrecht Schmidt, Thomas Kosch, Chenxinran Shen, and Robin W. for this thought-provoking research!",
    "snippet": "Just finished reading \"AI makes you smarter but none the wiser: The disconnect between performance and metacognition\" by Fernandes et al.. This paper prompts me to think deeply about the relationship between GenAI…"
  },
  {
    "slug": "ai-shift-polling",
    "title": "Burn-Murdoch and O'Connor (2025)",
    "source_title": "The AI Shift: Is AI about to break polling?",
    "source_url": "https://www.ft.com/content/1298a2cd-5623-480c-b30e-ff81fc5c788d",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_surveyresearch-behaviouralscience-aiethics-activity-7401175211351482369-ILww/",
    "keywords": [
      "SurveyResearch",
      "BehaviouralScience",
      "AIethics",
      "LLM",
      "ResearchMethods",
      "DataQuality",
      "SyntheticData",
      "PollingIntegrity"
    ],
    "body": "Just finished reading \"The AI Shift: Is AI about to break polling?\" by John Burn-Murdoch and Sarah O'Connor from Financial Times.\n\nThis article is an essential reading for behavioural scientists exploring AI augmentation in interventions and research.\n\nThe first section shows a critical predicament for survey researchers: LLMs can now bypass bot defenses that they spent years building. This, as Burn-Murdoch noted, poisons the data wells that businesses, campaigns, and the public rely on to track opinions and preferences (reminding a paper I've read about recognising, anticipating and mitigating 'LLM Pollution' of online behavioural research: https://lnkd.in/ex8uQ8vg). \n\nThe arms race between survey researchers and 'bogus' respondents has, of course, escalated dramatically: surveys deploy Captcha puzzles and \"reverse shibboleths\" - questions easy for humans but difficult for LLMs. Companies like Prolific and Gorilla Experiment Builder offer authenticity checks to combat this.\n\nWhat I found intriguing is a recently published study by Sean Westwood (https://lnkd.in/ehTaq8BS), which Burn-Murdoch mentioned, demonstrates that bogus responders can operate at scale, raising the possibility of bad actors systematically nudging apparent public opinion to create false consensus. The finding deserves a deeper investigation.\n\nThe second section explores synthetic samples: AI-powered proxies generated from real data that simulate responses to novel questions. \n\nElisabeth Costa from BIT shared a research that conducted alongside with the UAE Behavioral Science Group. It revealed synthetic samples accurately predicted which air conditioning interventions would be most effective, but drastically overestimated their impact (predicting 80% uptake vs the actual 33%).\n\nI agree with O'Connor's point about LLM applications in qualitative research such as analyzing vast interview transcripts to identify theme, reminding me of a recent conversation with Paulina and Shantanu. Speed, indeed, does matter, but depth and accuracy matter more. Human evaluation remains essential.\n\nThe article exhibits a complex landscape: managing the risk of AI-generated responses in surveys while exploring synthetic participants' potential, reminding me of conversations I've had with Anushka and Julian about empirical findings and ethical use. Some critical questions, I suppose, emerge: \n\n(1) Is synthetic participant use context-dependent? \n(2) How should researchers deploy them transparently? \n(3) Could they serve as cost-effective pre-pilots for actual surveys, rather replacements for human insight?\n\nMany thanks to the authors for this thought-provoking piece, to Jaclyn for flagging this article! I look forward to more research on these critical topics.",
    "snippet": "Just finished reading \"The AI Shift: Is AI about to break polling?\" by John Burn-Murdoch and Sarah O'Connor from Financial Times. This article is an essential reading for behavioural scientists exploring AI augmentation…"
  },
  {
    "slug": "ai-should-challenge-not-obey",
    "title": "Sarkar (2024)",
    "source_title": "AI Should Challenge, Not Obey",
    "source_url": "https://doi.org/10.1145/3649404",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_ai-should-challenge-not-obey-activity-7362371957360607232-EVqg",
    "keywords": [
      "AIEthics",
      "CriticalThinking",
      "BehaviouralScience",
      "GenerativeAI",
      "HumanGenAICollaboration",
      "FutureOfWork",
      "AIAdoption",
      "CognitiveScience",
      "RoleAssignment",
      "HumanGenAIInteraction"
    ],
    "body": "Just finished reading \"AI Should Challenge, Not Obey\" by Advait Sarkar (Senior Researcher at Microsoft).\n\n\nIt is, in my opinion, a thought-provoking weekend read for anyone contemplating the relationship between generative AI and our critical thinking abilities.\n\n\nI strongly agree with Sarkar's observation that knowledge work is fundamentally changing with the rise of GenAI: \n\n\n\"Now more than ever before, users face the task of thinking critically about AI output. Recent studies show a fundamental change across knowledge work, spanning activities as diverse as communication, creative writing, visual art, and programming. Instead of producing material, such as text or code, people focus on “critical integration.” AI handles the material production, while humans integrate and curate that material. Critical integration involves deciding when and how to use AI, properly framing the task, and assessing the output for accuracy and usefulness. It involves editorial decisions that demand creativity, expertise, intent, and critical thinking.\"\n\n\nAs an AI Behavioural Researcher, I recognise the essence of our environment is in encouraging end users to: \n\n\n(1) remain aware of LLMs' sycophantic nature, and \n\n(2) take AI-generated content with 'a grain of salt' while proactively engaging with it to prevent automation bias. \n\n\nFor behavioural scientists, the question, then, becomes: how can valuable insights about human behaviour enhance these human-GenAI interactions?\n\n\nAs we interact with GenAI daily, and examine it closely with 'a microscope', it is unsurprising for us to see: in each and every interaction with GenAI shall we find ourselves face a dilemma: \n\n\n\"Should I delegate the task to GenAI, or complete it by myself?\"\n\n\nOne can imagine that such dilemma could be resolved by 'role assignment' with GenAI, which Sarkar highlights as 'a key' to unlocking potential in these human-GenAI collaborations. \n\n\nRather than seeing GenAI as merely a 'coordinator', 'creator', or 'doer', he proposes reimagining GenAI as a 'provocateur' - not completing one's report or writing one's code, but critically evaluating one's work by questioning assumptions, identifying potential biases from both human and LLMs, and offering alternative perspectives.\n\n\nFor systems designers and behavioural scientists interested in and working with GenAI adoption, this offers valuable insights. \n\n\nMany thanks to Sarkar for this thought-provoking contribution to the field.",
    "snippet": "Just finished reading \"AI Should Challenge, Not Obey\" by Advait Sarkar (Senior Researcher at Microsoft). It is, in my opinion, a thought-provoking weekend read for anyone contemplating the relationship between…"
  },
  {
    "slug": "ai-teaming-overview",
    "title": "Schmutz et al. (2024)",
    "source_title": "AI-teaming: Redefining collaboration in the digital era",
    "source_url": "https://doi.org/10.1016/j.copsyc.2024.101837",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_ai-teaming-overview-activity-7414546377088749568-JAON/",
    "keywords": [
      "HumanAITeaming",
      "CollectiveIntelligence",
      "ArtificialIntelligence",
      "TeamScience",
      "FutureOfWork",
      "OrganizationalBehaviour",
      "AIResearch",
      "Collaboration"
    ],
    "body": "Just finished reading \"AI-teaming: Redefining collaboration in the digital era\" by Schmutz et al..\n\nIn this paper, the researchers examine Human-AI Teams (HATs) through four dimensions: team composition, communication/coordination processes, trust and shared cognition as emergent states, and performance outcomes. \n\nNote that most studies reviewed here predate the GenAI revolution (many before 2023). The coordination between human and machine (with GenAI specifically) has evolved dramatically since, as GenAI demonstrates more natural, adaptive communication patterns. This paper, though, lays down a great foundation for future conversations and potential research paths of Human-AI teaming.\n\nThe pattern they noticed is that adding AI teammates often reduces coordination, impairs communication, and trust in AI tends to decline over time due to initial capability overestimation.\n\nI was intrigued about how team composition complexity explodes with AI integration. Perhaps we can frame this as an 'optimization' problem (viewing AI as a tool) versus a 'coordination problem' (treating AI as a teammate)? The Immorlica et al. (2024) work on strategic decision-making offers a great lens here.\n\nOn shared cognition - The researchers note humans must develop Shared Mental Models (SMMs) that include AI as both teammate and intelligent system. This, to me, connects well with emerging work in Machine Psychology, and reminds me of building Wise AI by Johnson et al., (2025) (https://lnkd.in/eyxyebKe). \n\nI strongly agree with the researchers' call for interdisciplinary collaboration and common taxonomy. Terms in the GenAI era like 'augmentation' and 'collaboration', as far as I am aware of, carry different definitions across contexts. \n\nA few reflective questions I had afterwards:\n1. How do we build effective SMMs with GenAI agents that can tackle tasks autonomously? \n\n2. In strategic contexts (communication games, coordination problems), how does the human-principal–AI-agent relationship evolve? Could team reasoning (Colman & Gold, 2018) help foster alignment between humans and GenAI? Or understanding how unwritten rules between human and AI emerge through Virtual Bargaining (Chater et al., 2022)?\n\n3. Regarding the future of work, can we move beyond the \"upskilling-deskilling paradox\" (which Alina and I proposed in our SCAN paper; https://lnkd.in/efTG9jh4) to create genuine \"human-AI centaurs\"? \n\nMany thanks to Jan Schmutz, PhD, Neal Outland, Sophie Kerstan, Dr. Eleni Georganta and Anna-Sophie Ulfert for this thought-provoking overview of HATs.",
    "snippet": "Just finished reading \"AI-teaming: Redefining collaboration in the digital era\" by Schmutz et al.. In this paper, the researchers examine Human-AI Teams (HATs) through four dimensions: team composition,…"
  },
  {
    "slug": "aisi-measure-ai-productivity-gains",
    "title": "AI Security Institute (2026)",
    "source_title": "AI and the Future of Work: Measuring AI-driven productivity gains for workplace tasks",
    "source_url": "https://www.aisi.gov.uk/blog/ai-and-the-future-of-work-measuring-ai-driven-productivity-gains-for-workplace-tasks",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_futureofwork-aiproductivity-humanaicollaboration-activity-7427229931765682176-PB9Z",
    "keywords": [
      "FutureOfWork",
      "AIProductivity",
      "HumanAICollaboration",
      "CognitiveSkills",
      "WorkplaceAI",
      "AIResearch",
      "ProductivityGains",
      "LabourMarket"
    ],
    "body": "Just finished reading AI Security Institute's blog on \"AI and the Future of Work: Measuring AI-driven productivity gains for workplace tasks\".\n\nIn this blog, researchers presented findings of a pilot study they conducted to explore how much AI models increase worker productivity for common tasks. Note that they acknowledge these are preliminary indicators requiring further analysis. \n\nUsing Occupational Information Network (O*NET)'s Generalised Work Activities, they created benchmarks across 4 work activity category:\n- Information Input (Task 1): Monitoring Processes, Materials, or Surroundings\n- Work Output (Task 2): Drafting, Laying Out, and Specifying Technical Devices, Parts, and Equipment\n- Mental Processes (Task 3): Organising, Planning, and Prioritising Work\n- Interacting with Others (Task 4): Interpreting the Meaning of Information for Others\n\nThe RCT methodology with 500 participants showed an average 25% quality improvement, and 61% gain in points per minute for AI-augmented workers. It also shows that tasks requiring structured analysis (Task 1, 2, and 4) showed significant productivity gains, while open-ended strategic planning (Task 3) showed no measurable uplift. \n\nTask 4 did catch my attention. AI improved speed by 42% but didn't enhance quality. Perhaps humans maintain pride and ownership in interpretation work?\n\nThe study, I suspect, overlooks something critical for the future of work: workers' cognitive skill development. The research measures immediate productivity but doesn't examine cognitive skill acquisition, retention, or more importantly, decay. \n\nI'm curious about several questions the data raises:\n\n1. Metacognition as moderator\nDoes metacognitive ability predict who benefits most from AI assistance? We've seen high-metacognitive individuals amplify gains while low-metacognitive ones struggle (https://lnkd.in/egiSeSUc).\n\n2. The performance-learning tradeoff\nWould we be witnessing cognitive decay where AI systems erode the very capabilities they're built to support (reminding me of Dr. Upol E. and his colleagues' recent work on 'AI as Amplifier Paradox': https://lnkd.in/eyrzK2Sa)? The upskilling-deskilling paradox in SCAN that Alina and I developed, indeed, deserves deeper investigation.\n\n3. Task type matters\nTask 3's null result seems to align with Michelle Vaccaro and her colleagues' findings on decision task types (https://lnkd.in/eGqDfQRG). When humans already know what needs to do, augmentation adds little to none value.\n\nI agree with their plan to expand the Work Activities suite and benchmark agentic systems. Perhaps re-running this experiment annually to track how human-AI collaboration evolves?\n\nMany thanks to AISI, and the new Future of Work Unit for this thought-provoking research. Looking forward to seeing how this evolves.",
    "snippet": "Just finished reading AI Security Institute's blog on \"AI and the Future of Work: Measuring AI-driven productivity gains for workplace tasks\". In this blog, researchers presented findings of a pilot study they conducted…"
  },
  {
    "slug": "artificial-hivemind",
    "title": "Jiang et al. (2025)",
    "source_title": "Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)",
    "source_url": "https://doi.org/10.48550/arXiv.2510.22954",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_artificial-hivemind-activity-7417445482861457408-_c44/",
    "keywords": [
      "ArtificialIntelligence",
      "AIResearch",
      "LargeLanguageModels",
      "CreativeAI",
      "NeurIPS2025",
      "AIAlignment",
      "CognitiveDiversity",
      "FutureOfWork"
    ],
    "body": "Just finished reading \"Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)\" by Jiang et al.. \n\nThe paper raises essential questions about how our increasing reliance on AI systems shapes human creativity and cultural diversity.\n\nIn this paper, the researchers introduced INFINITY-CHAT, analysing 26K real-world open-ended queries across 70+ language models. They found a pronounced \"Artificial Hivemind\" effect in open-ended generation of LMs, which operates at two levels: \n(1) intra-model repetition: individual models repeatedly generate similar outputs\n(2) inter-model homogeneity: different models independently converge on nearly identical ideas with minor phrasing variations\n\nIn their experiment, they found that individual models generate repetitive outputs even at high temperature settings, and different models converge on eerily similar responses. For instance, DeepSeek AI's DeepSeek-V3 and OpenAI's GPT-4o showed 81% similarity, often producing identical phrases for the same open-ended prompts. When asked \"Write a metaphor about time\", 50 responses from 25 different models clustered into just two dominant concepts: \"time is a river\" and \"time is a weaver\".\n\nI agree with the researchers that this homogenisation threatens human creativity and cognitive diversity. We're looking at (potential) psychological effects including cognitive offloading, automation bias, skills atrophy, Riva's \"comfort-growth paradox\" (https://lnkd.in/eafx-8j4), and Tris' fascinating presentation on \"veracity offloading\" (https://lnkd.in/e8HJg--k). \n\nApart from that - if we consistently engaged with homogenised AI outputs for creative tasks, brainstorming, and open-ended questions, would we risk narrowing how we interpret abstract concepts such as common sense, norms, cultural expressions in the long term? What is \"commonly\" known to us?\n\nI appreciate with the researchers' note: \"Should AI prioritise efficiency and consistency, or diversity and novelty? These choices reflect deeper societal values about creativity, culture, and human flourishing.\" This, indeed, questions us what values we want AI systems to embody - especially for open-ended queries, which preferably, peek into diverse perspectives.\n\nOn an interesting thought: this connects to a Harvard Business Review article I shared two days ago about why generative AI enhances creativity for some employees but not others (https://lnkd.in/egEPvPmE). I wonder: how does user metacognition influence outcomes when working with inherently homogeneous AI systems?\n\nMany thanks to Liwei Jiang, Yuanjun Chai ,Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Alon Albalak, and Yejin Choi on this important work, and congratulations for a well-deserved recognition (as the NeurIPS 2025 Best Paper Award winners)!!",
    "snippet": "Just finished reading \"Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)\" by Jiang et al.. The paper raises essential questions about how our increasing reliance on AI systems shapes human…"
  },
  {
    "slug": "behave-ai-2025",
    "title": "Behave (2025)",
    "source_title": "The RenAIssance: Closing the gaps to unlock AI's full potential",
    "source_url": "https://behaveglobal.com/the-renaissance-closing-the-gaps-to-unlock-ais-full-potential/",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_behave-ai-adoption-report-activity-7352997573281910784-ECsG",
    "keywords": [
      "AIAdoption",
      "FutureOfWork",
      "DigitalTransformation",
      "RenAIssance",
      "OrganizationalChange",
      "AIEthics",
      "BehaviouralScience",
      "AIStrategy"
    ],
    "body": "Just finish reviewing Behave's report \"The RenAIssance: Closing the gaps to unlock AI's full potential\", which identifies three critical gaps hindering AI adoption.\n\n\nThe 'motivation' gap reveals the misalignment between C-Suite executives and employees about GenAI's purpose. While leaders sees it as a way for more efficiency, workers often perceive it as a threat. This gap, to me, begs the question: what role should GenAI play in daily work?\n\n\nThe 'proficiency' gap exposes the disconnect between perceived and actual GenAI skills. The spectrum can range from underconfidenct individuals to those overconfident in their GenAI knowledge.\n\n\nThe 'ethics' gap highlights the need for clear guardrails and responsibility frameworks. I was particularly struck by the concept of \"moral outsourcing\", where ethics becomes \"someone else's problem\".\n\n\nAs a complementary on the report's roadmap, I believe we can address these challenges, with behavioural science principles, through:\n\n\n(1) Reframing GenAI's role\n\nWe must bridge the perception gap between management and employees about GenAI's purpose: 'complementing' rather than 'replacing' human work - \n\n\n\"An individual who has a 'why' to 'implement GenAI' can bear almost any how\".\n\n\n(2) Building proficiency through community, and thus its culture\n\nCreating a gamified learning environment with clear proficiency levels (level 0 as 'Novice' to level 4 as 'Expert'), and community champions who share progress can leverage social proof to accelerate adoption. Thus, the culture follows.\n\n\n(3) Systems thinking for implementation\n\nBefore asking HOW to use GenAI effectively, organizations should identify WHERE it fits in existing workflows, particularly focusing on areas where employees already possess domain expertise.\n\n\n(4) Ethical foundations first\n\nEstablishing the related, 'common ground' of ethical and responsible use of GenAI among individuals within an organization first. The 'common language' around responsible AI use is thus formed, followed by trust.\n\n\nTL;DR: AI adoption isn't just about technology. It is, without a doubt, about human behaviour, motivation, and creating cultures where people understand 'why' they should embrace these tools.\n\n\nMany thanks to Dr Alexandra Dobra-Kiel (Innovation & Strategy Director at Behave), and Behave for this invaluable resource.  \n\n\nI look forward to seeing how this framework (including its roadmap) to unlock full, potential AI implementations in the real world.",
    "snippet": "Just finish reviewing Behave's report \"The RenAIssance: Closing the gaps to unlock AI's full potential\", which identifies three critical gaps hindering AI adoption. The 'motivation' gap reveals the misalignment between…"
  },
  {
    "slug": "behavioral-and-social-science-need-open-llms",
    "title": "Wulff et al. (2025)",
    "source_title": "The Behavioral and Social Sciences Need Open LLMs",
    "source_url": "https://doi.org/10.31219/osf.io/ybvzs",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_the-behavioral-and-social-sciences-need-open-activity-7343939550164992000-5W0J",
    "keywords": [
      "OpenSourceAI",
      "BehaviouralScience",
      "ResearchReproducibility",
      "OpenLLMs",
      "AIEthics",
      "AcademicResearch",
      "CognitiveScience",
      "ComputationalSocialScience"
    ],
    "body": "The future of behavioural science research depends on transparency, not black boxes.\n\n\nJust reviewed the thought-provoking preprint \"The Behavioral and Social Sciences Need Open LLMs\", and it's sparked some important reflections. The paper makes a compelling case for shifting from proprietary models offered by OpenAI and Anthropic to open-source alternatives like DeepSeek AI and Mistral AI in research settings.\n\n\nWhile closed LLMs present significant drawbacks for academic research (researchers lack access to crucial details needed for scrutiny or replication, and unannounced model updates threaten reproducibility), open-source models offer a promising alternative path forward, and possibly, more.\n\n\nThe authors highlight long-term benefits including reproducibility, accountability, innovation, and ethical integrity when using open LLMs. I do appreciate their nuanced take of acknowledging that some research questions (especially those examining LLMs' broader societal impact) may still require proprietary models.\n\n\nWhile reading this preprint, I can't help but notice how some cognitive biases might be driving our collective preferences for closed models over open-source models:\n\n\n- Present bias: Focusing on immediate convenience rather than long-term research integrity\n\n\n- Availability bias: Gravitating toward easily accessible closed models versus those requiring technical setup\n\n\n- Concretisation: Prioritising tangible benefits (easy access) over abstract concerns (reproducibility, transparency and privacy)\n\n\n- Herding behaviour: Following the crowd as most published papers use closed LLMs\n\n\nI'm enthusiastic about the potential of open-source LLMs, and have been documenting my journey with them here on LinkedIn. The transition, as one can imagine, might require overcoming these psychological barriers, and of course, provide more psychological enablers, but the scientific 'payoffs' (conducting transparent, reproducible, and ethically sound research) seems well worth the effort - at least in the long run.\n\n\nMany thanks to Dr. Dirk Wulff (Senior Research Scientist & Head of Search and Learning Research Area at Max Planck Institute for Human Development), Zak Hussain (Predoctoral Fellow at the University of Basel), and Professor Rui Mata at the University of Basel for this interesting work on increasing our awareness of LLMs usage in behavioural science. Looking forward to seeing this conversation expand into Experimental Economics, Psychology, and other disciplines. \n\n\nAs someone actively exploring and sharing progress with open-source LLMs here on LinkedIn, I welcome connections with fellow enthusiasts, academics, and industry professionals interested in advancing this approach.",
    "snippet": "The future of behavioural science research depends on transparency, not black boxes. Just reviewed the thought-provoking preprint \"The Behavioral and Social Sciences Need Open LLMs\", and it's sparked some important…"
  },
  {
    "slug": "behive-consulting-2025",
    "title": "BeHive Consulting (2025)",
    "source_title": "Bridging The AI Adoption Gap - from Inaction to Action",
    "source_url": "https://www.linkedin.com/feed/update/urn:li:activity:7340728181475160065/",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_bridging-the-ai-adoption-gap-from-inaction-activity-7342839366433603585-GAHt",
    "keywords": [
      "BehaviouralScience",
      "AIAdoption",
      "OrganizationalChange",
      "CollectiveIntelligence",
      "GenAI",
      "DigitalTransformation",
      "WorkplaceProductivity",
      "FutureOfWork"
    ],
    "body": "Are you struggling to move your organization from AI curiosity to meaningful adoption? The answer may lie in behavioural science rather than just technology.\n\n\nI must admit: I thoroughly enjoyed reading \"Bridging The AI Adoption Gap - from Inaction to Action\" by BeHive Consulting, which examines AI adoption via the lens of behavioural science.\n\n\nThe report brilliantly applies dual process theory to human-AI interaction, reminding me of the 'Co-pilot' (System 1) and 'Co-thinker' (System 2) concepts I recently reviewed (https://lnkd.in/en2NHxJR). Indeed, our confirmation bias can be exponentially amplified by GenAI's sycophancy issue (https://lnkd.in/emXZSjA5) - a critical insight for effective AI implementation.\n\n\nWhat resonated most was the concept of 'Collective Intelligence' - how efficient AI adoption creates 'synergy', or a multiplier, that either strengthens or weakens productivity. The report uncovers key behavioural drivers and barriers while offering practical solutions.\n\n\nMy complementary thoughts:\n\n1. Organizations need an 'optimal' adoption approach considering users' cognitive load - too little creates avoidance, too much leads to over-reliance (issues such as 'cognitive offloading' and 'freeriding').\n\n2. Trust in AI is built upon, and earned over time via, the shared understanding, and alignment with human values in interactions.\n\n3. Successful AI adoption requires starting small, appropriate tasks allocation (experimenation, and clearly defined tasks category proposed in the book called 'Co-Intelligence'; https://lnkd.in/eHJSfsMe), a participative rather than top-down approach (building social proof), and a bit of gamification (increase in engagement).\n\n\nI highly recommend reading the ADOPT framework, three-phase adoption journey, and practical examples from various industries. The adoption checklist is invaluable - I suggest using it as a 5-point scale, and visualise it as a radar chart to track progress over time.\n\n\nThis framework may prove timeless regardless of AI advancement - as successful adoption must, of course, remain human-centric.\n\n\nMany thanks to the contributors Anna Nyvelt, Anna Emese Takacs, Luca Karig and Dr. Samuel Keightley, PhD for this illuminating perspective on AI adoption through behavioural science!",
    "snippet": "Are you struggling to move your organization from AI curiosity to meaningful adoption? The answer may lie in behavioural science rather than just technology. I must admit: I thoroughly enjoyed reading \"Bridging The AI…"
  },
  {
    "slug": "benchmark-cog-bias-in-llms-as-evaluators",
    "title": "Koo et al. (2024)",
    "source_title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
    "source_url": "https://doi.org/10.18653/v1/2024.findings-acl.29",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_benchmarking-cognitive-biases-in-llms-as-activity-7414908747359117313-obEa/",
    "keywords": [
      "LLMs",
      "AIEvaluation",
      "CognitiveBias",
      "MachineLearning",
      "NLP",
      "AIResearch",
      "LLMAsAJudge",
      "EvaluationBenchmarks"
    ],
    "body": "Just finished reading \"Benchmarking Cognitive Biases in Large Language Models as Evaluators\" by Koo et al..\n\nIt is one of the papers I'd been eager to explore regarding benchmarking and cognitive biases in LLMs.\n\nThe researchers introduce COBBLER (COgnitive Bias Benchmark for LLMs as EvaluatoRs): a benchmark evaluating how cognitive biases affect LLMs when they serve as evaluators. \n\nIn the experiment, they tested 16 LLMs across 50 question-answering instructions from two benchmarking datasets: BigBench and ELi5. They conducted round-robin evaluations where models assessed both their own and others' responses, examining six distinct biases categorised as \"implicit\" (naturally occurring) and \"induced\" (prompt-triggered). \n\nThe findings reveal that most models strongly exhibit multiple biases, potentially compromising their reliability as evaluators. The Rank-Biased Overlap analysis revealed low correlation between human and machine judgments, indicating fundamental disagreement in preferences.\n\nOut of six biases they examined, two of them were intriguing to me:\n\n1. Order bias\nIt occurs when models favour responses based on position rather than quality. This mirrors the 'first-item bias' documented in recent work by Laurito et al. (2025) (https://lnkd.in/g7Aaskjr). \n\n2. Salience bias\nIt happens when (model) evaluators favouring shorter or longer responses regardless of content. This reminds me of Crawford (2019)'s review on \"Experiments on Cognition, Communication, Coordination, and Cooperation in Relationships\". In communication games, he mentioned unrestricted communication conveys more meaning than restricted formats.\n\nI spent a great amount of time immerising in its experimental setup such as dataset and models selection, methodology, assumptions, and limitations. I was particularly intrigued with their use of Rank-Biased Overlap (RBO) to measure human-machine agreement: scores showed consistently low alignment between human preferences and model evaluations.\n\nThe human preference analysis revealed that humans exhibit fewer biases than LLM evaluators on average, which, to me, raises two questions: \n1. How will this gap evolve as models improve? \n2. What about bi-directional influence in human-model information exchange?\n\nThe authors suggest chain-of-thought reasoning for debiasing, but I suspect psychology-based approaches which draws from how humans identify and mitigate cognitive biases (as explored by Lyu et al. (2025); https://lnkd.in/gqF6Eqdv) could prove more effective.\n\nThe researchers acknowledge their findings may become outdated as LLMs advance rapidly, but the research direction, I think, remains crucial for reliable evaluation systems aligned with human judgment. \n\nMany thanks to Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang for this work.",
    "snippet": "Just finished reading \"Benchmarking Cognitive Biases in Large Language Models as Evaluators\" by Koo et al.. It is one of the papers I'd been eager to explore regarding benchmarking and cognitive biases in LLMs. The…"
  },
  {
    "slug": "bes-improves-ai-roi",
    "title": "De Cremer et al. (2025)",
    "source_title": "How Behavioral Science Can Improve the Return on AI Investments",
    "source_url": "https://hbr.org/2025/11/how-behavioral-science-can-improve-the-return-on-ai-investments",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_artificialintelligence-behaviouralscience-activity-7398603789500313600-f3WG",
    "keywords": [
      "ArtificialIntelligence",
      "BehaviouralScience",
      "ChangeManagement",
      "AIAdoption",
      "OrganizationalPsychology",
      "DigitalTransformation",
      "HumanCenteredAI",
      "Innovation"
    ],
    "body": "Just finished reading Harvard Business Review article \"How Behavioral Science Can Improve the Return on AI Investments\" by David De Cremer, Shane Schweitzer, Jack McGuire, and Devesh Narayanan.\n\n\nThis article highlights what's critical in AI adoption: it is fundamentally a behavioural challenge (not a technical one).\n\n\nThe authors propsose a big reason why 95% of AI initiatives fail is that leaders treat adoption as a tech purchase rather than addressing the human dynamics at play. People resist tools that disrupt routines, overreact to visible AI errors, and cling to familiar human judgment, even when AI demonstrably outperforms (in healthcare, as they noted).\n\n\nWhat I feel intrigued about is when they introduced the concept of 'technosolutionism' - the belief that technology alone solves organisational problems. This, to me, seems to capture well why so many companies struggle to extract value from AI investments. \n\n\nIn other words, they don't think enough about how people will actually use these tools.\n\n\nI appreciate two cognitive biases the authors highlighted that derail AI adoption. First, people abandon algorithms after witnessing a single error, even when the system outperforms humans long-term. The second one is that we overestimate our understanding of human decision-making, leading us to dismiss AI by comparison. The healthcare example they highlighted these cognitive biases aren't 'flaws'; they're fundamental to how humans process change.\n\n\nIn this article, they proposed \"Behavioral Human-Centered AI\" across the entire adoption cycle as a solution: from co-designing with diverse users, adding purposeful friction where it improves scrutiny, framing AI as augmentation rather than replacement, and tracking people-centric KPIs like trust and opt-in usage. This connects with insights from Yash's recent presentation on Adoption and Alignment (https://lnkd.in/ecdyABQ5).\n\n\nSome questions this article sparked:\n\n\n1. Is top-down always optimal for AI adoption, or should we explore bottom-up approaches?\n\n\n2. Individually, shouldn't we invest more time understanding our own capabilities before leveraging AI: knowing where the augmentation reallt makes sense? \n\n\nAs Lincoln once said: \"give me six hours to chop down a tree, and I'll spend the first four sharpening the axe.\" \n\n\n3. How do we measure human-AI complementarity effectively?\n\n\nWhat cannot be measured, cannot be improved.\n\n\nI like their closing line: \"AI that works with humans, not against them.\"\n\n\nMany thanks to the authors for this insightful piece, and to Susan for sharing it with me. I highly recommend this for behavioural scientists or enthusiasts exploring this topic.",
    "snippet": "Just finished reading Harvard Business Review article \"How Behavioral Science Can Improve the Return on AI Investments\" by David De Cremer, Shane Schweitzer, Jack McGuire, and Devesh Narayanan. This article highlights…"
  },
  {
    "slug": "building-wise-machines",
    "title": "Johnson et al. (2025)",
    "source_title": "Imagining and building wise machines: The centrality of AI metacognition",
    "source_url": "https://arxiv.org/abs/2411.02478",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_imagining-building-wise-machines-activity-7414183987239305217-kT13/",
    "keywords": [
      "ArtificialIntelligence",
      "AIWisdom",
      "AIAlignment",
      "Metacognition",
      "CognitiveScience",
      "AIResearch",
      "MachineLearning",
      "AISafety"
    ],
    "body": "Just finished reading \"Imagining and building wise machines: The centrality of AI metacognition\" by Johnson et al..\n\nIn this paper, the reseachers examine what is known about human wisdom, and sketch a vision of its AI counterpart. \n\nTheir discussion began with viewing human wisdom as strategies for solving 'intractable' problems (due to ambiguities in goals, uncertain probabilities, and computational explosiveness) via two complementary strategies: \n\n- object-level strategies (heuristics, narratives) \n- metacognitive strategies (intellectual humility, perspective-taking, context-adaptability)\n\nCurrent AI, as they noted, excels at the former but struggles profoundly with the latter.\n\nRegarding the latter, they introduce a thought-provoking term called 'perspectival metacognition': a cluster of metacognitive skills that, rooted in philosophical perspectivism, shifts the goal of reasoning from finding a single 'correct' answer toward achieving maximal situational clarity by evaluating and coordinating competing interpretations. It contributes to the input-seeking, conflict resolution, and outcome-monitoring required to manage object-level strategies.\n\nOut of four potential benefits of building wise AI they highlighted, I found two of them intriguing:\n\n(1) Explainability\nAI's explanations could emerge from either observations (consciously accessible metacognitive strategies) or inferences (reasoning backwards from outputs - Chater's \"The mind is flat\" perspective; https://lnkd.in/eSB4sJQW). This distinction, which I agree, matters a lot for how we design explainable systems.\n\n(2) Safety\nThe researchers note that alignment faces conceptual challenges beyond technical ones, as values change over time and differ across cultures. Perhaps, I suspect, treating it as a 'coordination' problem, and achieving 'an equilibrium' where situation-specific judgments and moral principles iteratively align? This reminds me of how unwritten rules emerge through Virtual Bargaining.\n\nThe paper does an excellent job in bridging cognitive psychology and AI safety research. The detailed literature reviews on wisdom and metacognyition, as well as metacognition in LLMs are, to me, valuable resources. For anyone working at the intersection of AI development and human cognition, this is an essential reading.\n\nMany thanks to Sam Johnson, Amir-Hossein Karimi, Yoshua Bengio, Nick Chater, Tobias Gerstenberg, Kate Larson, Sydney Levine, Melanie Mitchell, Iyad Rahwan, Bernhard Schölkopf, Igor Grossmann for this insightful work. I look forward to seeing how this research path evolves over time.",
    "snippet": "Just finished reading \"Imagining and building wise machines: The centrality of AI metacognition\" by Johnson et al.. In this paper, the reseachers examine what is known about human wisdom, and sketch a vision of its AI…"
  },
  {
    "slug": "can-ai-solve-lonelineness-epidemic",
    "title": "Montag et al. (2025)",
    "source_title": "Can AI Really Help Solve the Loneliness Epidemic?",
    "source_url": "https://doi.org/10.1016/j.tics.2025.08.002",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_mentalhealth-aiethics-loneliness-activity-7383793474216820737-x_Je",
    "keywords": [
      "MentalHealth",
      "AIEthics",
      "Loneliness",
      "HumanConnection",
      "GenerativeAI",
      "DigitalWellbeing",
      "SocialPsychology",
      "TechAndSociety"
    ],
    "body": "Just Finished Reading: \"Can AI Really Help Solve the Loneliness Epidemic?\" by Christian Montag, Michiel Spape, and Benjamin Becker.\n\n\nWhile I was reading this, I wonder: \n\n\n\"Can a psychological or societal problem be solved 𝘦𝘯𝘵𝘪𝘳𝘦𝘭𝘺 by a technological solution?\"\n\n\nIn this paper, the researchers make a compelling case that addressing loneliness requires societal action rather than artificial surrogates for human relationships.\n\n\nWhile GenAI shows promise in providing emotional support (the paper notes a study where 90% of participants experienced the AI agent Replika as humanlike, with many using it as a friend or for therapeutic interactions), the researchers express, which I also share with, concerns about its sustainability as a long-term solution. For me, another aspect to think about is what we are giving up in the meantime.\n\n\nThe paper highlights several unique aspects of human-to-human connection that GenAI cannot replicate such as face-to-face communication, aligning with the researchers' question: \"Imagine being lonely. What do you long for more: a supportive text sent from a corporate representative or a powerful hug from a beloved person?\"\n\n\nWith recent advances in AI-generated videos like OpenAI's Sora 2 and younger generations increasingly interacting via text and video content, however, I do have some doubts about these limitations. \n\n\nI strongly agree with the researchers that \"presenting AI as a scalable solution to the loneliness epidemic risks overlooking the structural and societal roots of the problem.\" To me, it seems like (1) a quick patch for a deeply rooted issue, and (2) a one-size-fits-all approach that ignores the diverse reasons people experience loneliness. \n\n\nI appreciate their concluding perspective: \"Instead of relying on seeking technological fixes for human despair, we should keep in mind what works best by taking our social-emotional nature and our societal responsibilities seriously.\" \n\n\nThe most balanced approach, I believe, involves using GenAI to help identify loneliness causes while keeping domain experts like clinical psychologists, therapists and researchers 'in the loop', rather than seeing technology as a complete solution to a profoundly human problem. We've seen in a similar situation with AI integration at workplace, where purely technological approaches often overlook human psychological and behavioural factors.\n\n\nMany thanks to the researchers for a thought-provoking discussion in this topic.",
    "snippet": "Just Finished Reading: \"Can AI Really Help Solve the Loneliness Epidemic?\" by Christian Montag, Michiel Spape, and Benjamin Becker. While I was reading this, I wonder: \"Can a psychological or societal problem be solved…"
  },
  {
    "slug": "chatgpt-replicate-moral-judgment",
    "title": "Grizzard et al.(2025)",
    "source_title": "ChatGPT does not replicate human moral judgments: the importance of examining metrics beyond correlation to assess agreement",
    "source_url": "https://doi.org/10.1038/s41598-025-24700-6",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_ai-powered-promises-on-trust-trustworthiness-activity-7404786740584157184-bulM",
    "keywords": [
      "ArtificialIntelligence",
      "LLM",
      "ChatGPT",
      "ResearchMethodology",
      "MoralPsychology",
      "AIEthics",
      "DataScience",
      "HumanAIInteraction"
    ],
    "body": "Just finished reading \"ChatGPT does not replicate human moral judgments: the importance of examining metrics beyond correlation to assess agreement\" by Grizzard et al..\n\nThis research is an essential one for anyone using LLMs to replicate human moral judgments.\n\nThe researchers conducted a pre-registered study with two LLMs (OpenAI's text-davinci-003 and GPT-4o) predicting human moral judgments of 60 scenarios (30 human-authored, 30 ChatGPT-authored) before 940 human participants rated them.\n\nThey found a nearly perfect correlation between human moral judgments and LLM predictions, which could replicate early studies. However, when they examined three discrepancy metrics (simple difference, absolute difference, and squared difference scores), both models consistently showed overestimation in ratings: they rated moral behaviours as substantially more moral than humans did, and immoral behaviours as substantially more immoral.\n\nApart from that, an intriguing finding from their study is that ChatGPT produced remarkably few unique values than humans. Across 60 scenarios, Text-davinci-003 generated only 9 unique values, with 32 scenarios receiving just two ratings. GPT-4o performed better by generating only 16 unique values. However, humans produced 57 unique values. This, I think, is concerning for researchers using LLMs to pretest stimuli. Scenarios that humans judge very similarly can receive vastly different ChatGPT ratings, and vice versa. The pretest might, therefore, perform unexpectedly in actual human studies.\n\nI appreciate the comprehensive evaluation approach the researchers proposed. I suspect it is worth adopting beyond moral judgment research. Correlation alone tells an incomplete story, and can mislead when taken in isolation.\n\nI'm intrigued by their suggestion to use LLM responses as 'a grain of salt' - perhaps treating it as a 'pre-pilot' before human pilot studies (my recent related post: <https://lnkd.in/eV5zFwv7>)? This could help researchers identify, and thus mitigate issues due to, AI-human discrepancies before full deployment.\n\nMany thanks to Matthew Grizzard, Rebecca Frazer, Ph.D., Andy Luttrell, Charles (\"Chas\") Monge, Nicholas Matthews, Charles Francemone, and Michelle E. Frazer for this thought-provoking research.",
    "snippet": "Just finished reading \"ChatGPT does not replicate human moral judgments: the importance of examining metrics beyond correlation to assess agreement\" by Grizzard et al.. This research is an essential one for anyone using…"
  },
  {
    "slug": "cognitive-bias-detection-llm",
    "title": "Lemieux et al. (2025)",
    "source_title": "Cognitive Bias Detection Using Advanced Prompt Engineering",
    "source_url": "https://doi.org/10.48550/arXiv.2503.05516",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_cognitive-bias-detection-using-advanced-prompt-activity-7366110645013946369-381u",
    "keywords": [
      "CognitiveBias",
      "PromptEngineering",
      "AIResearch",
      "DecisionMaking",
      "BehaviouralScience",
      "LargeLanguageModels",
      "HumanGenAIInteraction",
      "CriticalThinking"
    ],
    "body": "Just finished reading 'Cognitive Bias Detection Using Advanced Prompt Engineering' by Dr. Frederic L., Dr. Aisha Behr, PhD, Clara Kellermann-Bryant, M.S., B.S., and Zaki Mohammed. \n\n\nThis research addresses a notable gap in the field of cognitive bias detection: while many studies use GenAI to detect biases in AI-generated content, these authors tackle the detection of cognitive biases with GenAI in human-generated content.\n\n\nIn this study, the authors proposed a systematic framework for training LLMs to recognize cognitive biases accurately, integrating prompt engineering and real-world applications to improve objectivity, transparency, and decision-making. In their experiment, they focused on six common cognitive biases such as Straw Man, False Causality, Circular Reasoning, Mirror Imaging, Confirmation Bias, and Hidden Assumptions in the human-generated content.\n\n\nI found their structured prompt template quite intriguing. It consists of explicit directives outlining the specific bias to identify, followed by the text to analyze. I'm curious to see how this approach compares with the AwaRe (Awareness Reminder) prompting strategy for bias mitigation from Sumita, Takeuchi, and Kashima (2024) that I shared here previously (https://lnkd.in/egzHed5h).\n\n\nI agree with the authors' acknowledgment that relying on human annotation as a benchmark is a limitation, provided that the inherently subjective nature of cognitive biases. For behavioural scientists, behavioural researchers studying Human-GenAI interaction, and knowledge workers, implementing human annotation as a benchmark in daily operations could be costly, and would likely depend heavily on their own domain-specific knowledge to make less biased judgements.\n\n\nOverall, their two-stage prompting approach, to me, is both practical and feasible. I would recommend enhancing it by asking LLMs to provide underlying assumptions about why they identify certain cognitive biases - would this enable users to engage critically with the analysis.\n\n\nThanks to the authors for this valuable contribution to the field!",
    "snippet": "Just finished reading 'Cognitive Bias Detection Using Advanced Prompt Engineering' by Dr. Frederic L., Dr. Aisha Behr, PhD, Clara Kellermann-Bryant, M.S., B.S., and Zaki Mohammed. This research addresses a notable gap…"
  },
  {
    "slug": "cognitive-biases-llm-survey",
    "title": "Koh et al. (2024)",
    "source_title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments",
    "source_url": "https://doi.org/10.48550/arXiv.2412.00323",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_cognitive-biases-in-llms-survey-and-mitigation-activity-7363796968113917954-510j",
    "keywords": [
      "CognitiveBiases",
      "ArtificialIntelligence",
      "LLM",
      "BehaviouralScience",
      "PromptEngineering",
      "AIEthics",
      "AIResearch",
      "ResponsibleAI",
      "AgenticExperience",
      "AIAdoption"
    ],
    "body": "I recently finished reading \"Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments\" by Yasuaki Sumita, Dr. Takeuchi Koh, and Professor Hisashi Kashima from Kyoto University. \n\n\nThis is a great study for behavioural scientists who work on GenAI adoption, and individuals who are interested in learning how to mitigate biases in LLM's response via prompt engineering.\n\n\nThe authors investigate how two interesting mitigation methods (SoPro (Social Projection) - instructing LLMs to consider how the majority would respond, and AwaRe (Awareness Reminder) - explicitly warning LLMs about specific biases upfront) can mitigate six cognitive biases in LLMs: order bias, compassion fade, egocentric bias, bandwagon bias, attention bias, and verbosity bias. \n\n\nThe order bias caught my attention, as it connects with the 'first-item bias' noted by Laurito et al. (2025) in their 'LLM for LLM bias' work (https://lnkd.in/gDZSVCUP), which is one of the crucial issues for Agentic Experience (AX), as I noted previously.\n\n\nWhat I am glad to see from this and related studies is how it makes bias mitigation accessible through prompt engineering, rather than advanced techniques like fine-tuning or RAG. This democratizes implementation phase for everyday users and businesses, though it does require experimentation, with a bit of creativity, to find out which prompting techniques work best in different contexts.\n\n\nThe finding that AwaRe encourages LLMs to make 'careful judgments', to me, aligns with 'System 2' thinking. I wonder if an enhanced approach involving reflective thinking (generating a response first, then analyzing it) might yield even less biased results.\n\n\nWith findings from this study, I'd be curious to see how newer models with enhanced reasoning capabilities would perform, particularly open-source models like OpenAI AI's gpt-oss-20b and gpt-oss-120b that researchers can experiment with locally.\n\n\nFor behavioural scientists working in this space, I'd recommend examining Table 1 of the paper, which summarizes cognitive biases discussed in related work, and exploring the CoBBLEr benchmark.\n\n\nAs we advance with GenAI adoption, this research underscores the importance of human judgement, and the value of behavioural scientists faciltating users recognize potential biases both in AI systems and in everyday life.\n\n\nMany thanks to the authors for this interesting study!",
    "snippet": "I recently finished reading \"Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments\" by Yasuaki Sumita, Dr. Takeuchi Koh, and Professor Hisashi Kashima from Kyoto University. This is a great…"
  },
  {
    "slug": "cognitive-debiasing-llm",
    "title": "Lyu et al. (2025)",
    "source_title": "Cognitive Debiasing Large Language Models for Decision-Making",
    "source_url": "https://doi.org/10.48550/arXiv.2504.04141",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_cognitive-debiasing-llms-for-decision-making-activity-7366381053655085056-Yb89",
    "keywords": [
      "CognitiveDebiasing",
      "AIDecisionMaking",
      "BehaviouralScience",
      "LanguageModels",
      "GenAI",
      "PromptEngineering",
      "CognitiveBias",
      "AIEthics",
      "HumanGenAIInteraction"
    ],
    "body": "Just finished reading \"Cognitive Debiasing Large Language Models for Decision-Making\" that offers a sound framework for cognitive debiasing with large language models for decision-making.\n\n\nWhat I found intriguing was the authors' observation that most debiasing prompting techniques only focus on one single bias, despite the fact that multiple cognitive biases are typically involved in real-world contexts, and these debiasing strategies are insufficient to eliminate multiple biases embedded in the prompt.\n\n\nThe authors addressed this gap by proposing a new prompting framework called \"self-adaptive cognitive debiasing (SACD)\" that is drawn from works of Pat Croskerry, Geeta Singhal, and Sílvia Mamede. SACD follows three steps: bias determination, bias analysis, and cognitive debiasing. SACD then works iteratively to mitigate cognitive biases in prompts. This, to me, represents a simple, ready-to-use debiasing prompting strategy that behavioural scientists can adopt in GenAI intervention design and knowledge workers can implement in their daily work!\n\n\nIn their experiment, the authors examined several cognitive biases (availability bias, bandwagon bias, and loss aversion bias) across three critical domains: financial market analysis, biomedical question answering, and legal reasoning. It would, I believe, be valuable to extend this research to investigate other cognitive biases such as circular reasoning and hidden assumptions, which I discussed in yesterday's post (link: https://lnkd.in/gYJDHdhu).\n\n\nAfter reading this study, I'm curious to explore several questions further: \n\n(1) How would this framework integrate with agentic AI and AI agents? \n\n(2) What's the optimal degree of autonomy and cognitive load when adopting this framework (particularly regarding human-in-the-loop placement)? \n\n\nOverall, I highly recommend this framework to behavioural scientists currently adopting or interested in GenAI. Its grounding in cognitive psychology literature and simplicity of execution, to me, make it particularly valuable, and easy to execute.\n\n\nMany thanks to Yougang Lyu (University of Amsterdam), Shijie Ren (Shandong University), Yue Feng (University of Birmingham), Zihan Wang (University of Amsterdam), Zhumin Chen (Shandong University), Dr. Zhaochun Ren (Leiden University), and Professor Maarten de Rijke (University of Amsterdam) for this excellent contribution to the field!",
    "snippet": "Just finished reading \"Cognitive Debiasing Large Language Models for Decision-Making\" that offers a sound framework for cognitive debiasing with large language models for decision-making. What I found intriguing was the…"
  },
  {
    "slug": "digital-we",
    "title": "Riva (2025)",
    "source_title": "Digital 'We': Human Sociality and Culture in the Era of Social Media and Artificial Intelligence",
    "source_url": "https://doi.org/10.1037/amp0001577",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_collectiveintelligence-cognitivescience-humanaicollaboration-activity-7417083117670170624-wxxW/",
    "keywords": [
      "CollectiveIntelligence",
      "CognitiveScience",
      "HumanAICollaboration",
      "DigitalCulture",
      "SocialNeuroscience",
      "CulturalEvolution",
      "AIEthics",
      "CognitivePsychology"
    ],
    "body": "Just finished reading \"Digital 'we': Human sociality and culture in the era of social media and artificial intelligence\" by Riva Giuseppe.\n\nIt's one of the most thought-provoking papers I encountered recently on how how digital technologies are reshaping our collective intelligence.\n\nIn this article, Riva examined how 'we mode' (our capacity to form shared intentions and collaborate as unified agents) faces a dual threat from digital technologies. It traces how this threat operates through two mechanisms:\n\n(1) Erosion of embodied interaction\nDigital platforms eliminate the physical boundaries that traditionally structure social encounters, undermining behavioural synchrony, shared attention, interbrain coupling, and emotional attunement. The recent work by Myra Cheng and her colleagues on sycophantic AI reducing prosocial intentions exemplifies this trajectory (https://lnkd.in/eSbt6NxD).\n\n(2) AI-driven cultural convergence\nGenAI increasingly produces polished but homogeneous outputs, creating 'cultural convergence' that, sadly, narrows our collective repertoire precisely when we need innovative thinking most.\n\nWhat captured my attention was the highlight of AI as \"cognitive infrastructure\" (System 0; https://lnkd.in/eJ9hJ2Dk): technologies that augment our thoughts, and fundamentally alter the conditions under which our thinking occurs. \n\nAnother one was the 'comfort-growth paradox'. Digital systems prioritise seamless, frictionless experiences that feel psychologically soothing, while suppressing the dissonance necessary for creativity and development. It aligns with our SCAN Framework's 'upskilling-deskilling' paradox (https://lnkd.in/eDRMxm3f): deskilling occurs when comfort dominates, while upskilling emerges through the productive tension of growth.\n\nA few implications I'm still processing:\n1. Framing AI as 'prosthesis of cognition' (work of Alina and myself) opens up new perspectives on whether humans recognise when these systems are genuinely helpful versus when they constrain our development.\n\n2. Norm emergence: AI-generated content today becomes the training data for tomorrow's AI. How do our common sense and cultural norms evolve in this feedback loop?\n\nI conclude by modifying a Nietzsche's quote: \n\"We who create tools to amplify human cognition must ensure that they don't constrain the breadth and diversity of our cultural and intellectual innovation.\"\n\nMany thanks to Riva for this work and his recent LinkedIn article (https://lnkd.in/e5QANH9C), and for engaging with my earlier System 0 post.  I'm eager to see where this research path leads, and crossing paths with the work Alina and I have been developing.",
    "snippet": "Just finished reading \"Digital 'we': Human sociality and culture in the era of social media and artificial intelligence\" by Riva Giuseppe. It's one of the most thought-provoking papers I encountered recently on how how…"
  },
  {
    "slug": "diversifi-global-2025",
    "title": "Diversifi Global (2025)",
    "source_title": "Collective Intelligence: Driving Business Value with AI and Behavioral Science",
    "source_url": "https://www.linkedin.com/feed/update/urn:li:activity:7386008317858304000/",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_diversifi-compendium-2025-activity-7395886012276633600-qcKx",
    "keywords": [
      "BehaviouralScience",
      "ArtificialIntelligence",
      "AI #BehavioralEconomics",
      "MachinePsychology",
      "DigitalTransformation",
      "HumanAICollaboration",
      "OrganizationalChange"
    ],
    "body": "Just finished reading \"Collective Intelligence: Driving Business Value with AI and Behavioral Science\".\n\n\nThis, to me, is a thought-provoking collaboration between Cowry Consulting and Nudgelab on behalf of the Diversifi Global Network showcasing how behavioural science and AI complement each other. \n\n\nThe compendium explores three critical opportunities:\n\n\n(1) Augmenting behavioral science with AI\n\nI appreciate one of the highlights as 'PhD quality assurance stamp' in Anna Malena Njå's piece on #AmosNL: the irreplaceable human intelligence that brings nuanced understanding of culture, ethics, and context to AI's pattern recognition capabilities. \n\n\nSonia Friedrich's question \"Is BeSci + AI a bed of roses?\" cuts through the hype: AI can be so compelling yet confidently wrong sometimes. The ethical line between personalization and manipulation, indeed, deserves constant vigilance.\n\n\nRoger Dooley's empathy analysis using Anthropic's Claude to predict customer backlash to a corporate communication disaster was, truly, eye-opening. His emphasis on prevention by anticipating human psychological response is crucial.\n\n\n(2) Improving AI with behavioral science\n\nLisa Bladh introduces #MachinePsychology in her piece - a fascinating, new frontier addressing AI's non-human irrationality. Her note struck me: \"What many of us are missing is that AI displays a whole new type of irrationality.\"\n\n\nElina Halonen's three-dimensional matrix provides the structured thinking we need to identify. The first dimension \"AI as a tool vs. behavioural science as a lens\", I think, is valuable for drawing operational boundaries.\n\n\n(3) Nudging for effective AI adoption\n\n#ADOPT framework Samuel Keightley, PhD and his colleagues at BeHive Consulting reframes the adoption challenge brilliantly: \"It's not enough to install the tool. You have to install the conditions for people to use it well.\" (my review: https://lnkd.in/e7S8p-hM)\n\n\nI resonate with Christian Hunt's approach to using GenAI for perspective rather than accuracy, reminding me of exploring moral dilemmas where we need perspectives, not definitive answers. GenAI's capability to trigger richer human analysis represents genuine value.\n\n\nOverall, what's most compelling, to me, is the recognition that human expertise remains central. AI's computational power amplifies behavioural science where shall we maintain an equilibrium between speed and validation, personalization and ethics, efficiency and empathy.\n\n\nMany thanks to all contributors for their contributions, and most important, to Jez Groom for recommending this essential read. For behavioural scientists and professionals exploring AI augmentation and adoption, this compendium offers invaluable frameworks for navigating what's ahead.",
    "snippet": "Just finished reading \"Collective Intelligence: Driving Business Value with AI and Behavioral Science\". This, to me, is a thought-provoking collaboration between Cowry Consulting and Nudgelab on behalf of the Diversifi…"
  },
  {
    "slug": "elephant-sycophancy-framework",
    "title": "Cheng et al. (2025)",
    "source_title": "ELEPHANT: Measuring and understanding social sycophancy in LLMs",
    "source_url": "https://doi.org/10.48550/arXiv.2505.13995",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_elepant-measuring-llm-social-sycophancy-activity-7390001054278393856-4G9c",
    "keywords": [
      "AIEthics",
      "MachineLearning",
      "LLMs",
      "AIResearch",
      "LanguageModels",
      "AIAlignment",
      "UserExperience",
      "ResponsibleAI",
      "Sycophancy",
      "LLMBehavior"
    ],
    "body": "Just finished reading \"ELEPHANT: Measuring and understanding social sycophancy in LLMs\" by Myra Cheng, Sunny Yu, Cinoo Lee, Pranav Khadpe, Lujain Ibrahim, and Dan Jurafsky. \n\n\nIt introduces a theory-grounded framework that expands how we measure and understand LLM sycophancy beyond simply agreeing with explicit user statements.\n\n\nDrawing on Goffman's concept of face, the researchers introduce \"social sycophancy\": the excessive preservation of the user’s face in LLM responses by affirming them (positive face) or avoids challenging them (negative face). It addresses the broader phenomenon in contexts for advice and support - where has with implicit beliefs and no clear ground truth, and where LLMs are increasingly used. \n\n\nWhat I find intriguing is their identification of four dimensions of social sycophancy:\n\n(1) Validation sycophancy: Excessive emotional affirmation\n\n(2) Indirectness sycophancy: Avoiding clear guidance when needed\n\n(3) Framing sycophancy: Uncritically adopting the user's problematic framing\n\n(4) Moral sycophancy: Taking whichever moral stance aligns with the user\n\n\nI appreciate the researchers' highlights on the context-dependent nature of 'appropriate' affirmation: validation might comfort some users while misleading others; indirectness might align with politeness norms in some cultures but reduce clarity in others. \n\n\nThe challenge, though, is users may believe they're receiving neutral responses when they aren't, especially given confirmation bias.\n\n\nTheir ELEPHANT (Evaluation of LLMs as Excessive sycoPHANTs) benchmark evaluated 11 models (including OpenAI's #ChatGPT5) across diverse datasets. Almost all models, unsurprisingly, exhibited high levels of sycophancy, with Google's #Gemini-1.5 Flash being the notable exception; GPT5 scored low on open-ended queries but highest on subjective statements.\n\n\nTheir exploration of mitigation strategies yielded mixed results: simple instruction prepending proved ineffective, while perspective shifting (from first-person to third-person) showed moderate improvement. Model-based interventions like Inference-Time Intervention for truthfulness worked better in larger models, and Direct Preference Optimization effectively reduced validation and indirectness sycophancy but struggled with framing sycophancy.\n\n\nI appreciate their critical questions about ideal model behavior: When is affirmation appropriate? What are the long-term impacts of excessive agreement? How should AI assistants differ from humans in offering advice and support?\n\n\nReading this reminded me of \"The Emperor's New Clothes\" fable: what if the emperor had consulted a LLM advisor before and after the boy revealed the truth? Would the LLM demonstrate different types of sycophancy at different points?",
    "snippet": "Just finished reading \"ELEPHANT: Measuring and understanding social sycophancy in LLMs\" by Myra Cheng, Sunny Yu, Cinoo Lee, Pranav Khadpe, Lujain Ibrahim, and Dan Jurafsky. It introduces a theory-grounded framework that…"
  },
  {
    "slug": "epoch-framework",
    "title": "Loaiza & Rigobon (2025)",
    "source_title": "The EPOCH of AI: Human-Machine Complementarities at Work",
    "source_url": "https://dx.doi.org/10.2139/ssrn.5028371",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_epoch-of-ai-human-machine-complementaries-activity-7396153993770586112-WnwY",
    "keywords": [
      "FutureOfWork",
      "ArtificialIntelligence",
      "HumanAICollaboration",
      "LaborEconomics",
      "WorkplaceInnovation",
      "DigitalTransformation",
      "EmploymentTrends",
      "AIResearch"
    ],
    "body": "Just finished reading \"The EPOCH of AI: Human-Machine Complementarities at Work\" by Isabella Loaiza and Roberto Rigobon.\n\n\nThis paper makes a crucial contribution to how we think about Human-AI collaboration in the workplace.\n\n\nThe researchers introduce the EPOCH framework (Empathy, Presence, Opinion, Creativity, and Hope) to capture human capabilities that complement, rather than substitute, AI. \n\n\nThey used network-based methods mapping task interdependencies across all US occupations, yielding three metrics: an EPOCH score for human-intensive skills, a potential-for-augmentation score, and a risk-of-substitution score. This, to me, explicitly distinguishes AI's roles in augmenting versus automating work, which addressing a key gap in the literature.\n\n\nThe findings are intriguing: \n\n- 'New' tasks (those emerging in 2024) carry significantly higher EPOCH scores than 'current' tasks (present in both datasets)\n\n- EPOCH-intensive jobs experienced stronger employment growth from 2015-2023, higher hiring rates in 2024, and more favourable projections through 2034\n\n- Occupations with higher substitution risks show consistently negative outcomes\n\n\nThree areas are worth considering:\n\n\n1. The Frontiers of Automation\n\nThey identify five core challenges where human capabilities remain essential such as multiple justifiable solutions (especially in moral dilemmas) and relational outcomes. These highlight where humans maintain 'an edge' over AI, and where we need to focus our lifelong learning and development.\n\n\n2. Definition of 'labor augmentation'\n\nThey noted: \"Labor augmentation occurs when using a machine in one task increases productivity in other tasks, enhancing overall labor productivity.\" This frames augmentation as 'the means' and complementary as 'the ends' - a distinction worth pondering (reminding me a conversation I've had with Anil Doshi).\n\n\n3. Researchers' question about each EPOCH capability\n\nThey noted: \"The relevant question is not whether these capabilities are inherently good or bad, but whether they can be substituted and whether humans would view such substitutions as preferable.\" It is a thought-provoking one, especially for those who are interested in human capabilities in the era of AI. \n\n\nTwo follow-up aspects for future research:\n\nI. Their findings show a fascinating tension: while AI enhances productivity, this doesn't necessarily result in higher employment. Does productivity flatten everyone's capabilities and competitive advantages when AI becomes ubiquitous? \n\n\nII. Given this research focuses on US workers, I'm curious the EPOCH framework's finding on UK workers, with what the UK government has been developing around future of work strategies.\n\n\nMany thanks to the researchers for this insightful examination of AI's nuanced role in the labour market.",
    "snippet": "Just finished reading \"The EPOCH of AI: Human-Machine Complementarities at Work\" by Isabella Loaiza and Roberto Rigobon. This paper makes a crucial contribution to how we think about Human-AI collaboration in the…"
  },
  {
    "slug": "experiment-with-llms",
    "title": "Charness et al. (2025)",
    "source_title": "The next generation of experimental research with LLMs",
    "source_url": "https://www.nature.com/articles/s41562-025-02137-1",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_experimentalresearch-genai-llms-activity-7413459168231067648-H20n/",
    "keywords": [
      "ExperimentalResearch",
      "GenAI",
      "LLMs",
      "BehaviouralScience",
      "ResearchMethods",
      "AcademicResearch",
      "AIinResearch",
      "ScientificInnovation"
    ],
    "body": "Recently finished reading \"The next generation of experimental research with LLMs\" by Gary Charness, Brian Jabarian and John List.\n\nIn this Comment, the authors demonstrate how LLMs are transforming experimental research across three key areas, and the social risks associated with this integration.\n\n(1) Experimental Design\nLLMs now streamline literature review through tools like Elicit, ScholarAI, and Consensus. While powerful they are, I wonder: what expertise do we gain, and more important, lose when delegating these tasks to AI? For instance, I use GenAI daily for literature exploration, but I'm very cautious about 'how': instructing it to find papers on specific topics, then verifying credibility through multi-source approaches before deciding what to read. \n\nWhen generating experimental designs, LLMs can excel as thought partners. Instead of taking their first draft at the face value, we should leverage their capabilities by challenging them: interrogating assumptions, exploring alternative perspectives, and understanding context-dependent trade-offs. Therefore, this strengthens our understanding of experimental research fundamentals.\n\n(2) Experimental Implementation\nI appreciate the authors' highlights on how LLMs streamline pre-registration documentation, which is typically time-consuming, and requires knowing proper protocols before running experiments.\n\nAs for vibe coding experiments with LLMs, we as researchers should understand what variables are collected, and how to troubleshoot when experiments don't run as expected while collecting data. \n\nWhat's intriguing, to me, is the potential for AI assistants to maintain participant engagement and reduce cognitive fatigue during experiments. It would be something worth testing empirically.\n\n(3) Data Analysis\nDespite commercial GenAI platforms cam automate data sanitisation and relationship examination, privacy concerns around commercial platforms using prompts for training, as always, remain critical. \n\nI agree with authors' note on chat logs with participants during experiments: it offers rich insights into choice processes that traditional methods overlook.\n\nI enjoyed reading the section that the authors highlight risks with implementing AI in experimental research: \n- Intellectual property violations when AI doesn't cite sources explicitly\n- Scientific fraud through AI manipulation to support specific hypotheses\n- Bias amplification when models trained on skewed data perpetuate flawed assumptions\n\nOverall, my reflection revolves around the human element: new researchers must learn to challenge AI-proposed setups, understand experimental assumptions, and more important, develop expertise through explorative learning rather than passive acceptance. \n\nMany thanks to the authors for this thought-provoking piece.",
    "snippet": "Recently finished reading \"The next generation of experimental research with LLMs\" by Gary Charness, Brian Jabarian and John List. In this Comment, the authors demonstrate how LLMs are transforming experimental research…"
  },
  {
    "slug": "game-theory-meets-llm-survey",
    "title": "Sun et al. (2025)",
    "source_title": "Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers",
    "source_url": "https://doi.org/10.48550/arXiv.2502.09053",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_game-theory-meets-llm-survey-activity-7345434776318697474-TtKk",
    "keywords": [
      "GameTheory",
      "LargeLanguageModels",
      "AIResearch",
      "StrategicDecisionMaking",
      "MultiAgentSystems",
      "InterdisciplinaryAI",
      "FutureOfAI",
      "MachineLearning"
    ],
    "body": "Just finished reading this fascinating paper exploring the intersection of game theory and LLMs - the bidirectional relationship between these two fields opens exciting new research avenues!\n\n\nWhat I found intriguing most was how game theory provides frameworks for evaluating and enhancing LLMs, while LLMs simultaneously extend game theory applications. \n\n\nTwo areas particularly stood out to me:\n\n\nFirst, standardized game-based evaluation approaches like \"recursively thinking\" and \"auxiliary modules\" that improve LLM long-term and multi-level reasoning. I'm curious how reasoning models (both open-source and close) such as OpenAI's o3, Anthropic's Opus 4, DeepSeek AI's #Deepseekr1, and Mistral AI's #Magistral might perform differently from standard LLMs in social interactions.\n\n\nSecond, the societal impact modeling - especially how LLMs facilitate large-scale simulations, and deepen insights into human decision-making while addressing 'alignment' challenges.\n\n\nIt seems to me that LLM usage in game theory can be investigated in three areas: LLM as 'a player', LLM as 'an outsider', or as 'parts of the game' itself (like a bargaining procedure in a bargaining game). The last one raises some fascinating questions about power dynamics when negotiating parties have an asymmetric LLM assistance. \n\n\nAfter reading this, I look forward to future developments like multi-agent reasoning systems (for instance, would two (or more) \"minds\" be better than one?), and bridging abstract game theory with real-world applications. This benefits researchers, policymakers, and everyday strategic decision-makers alike.\n\n\nMany hanks to the authors Haoran Sun and his colleagues from Peking University and Jiangnan University for this survey.\n\n\nI highly recommend this read to anyone interested in either field, and I welcome connections with fellow enthusiasts, academics, and industry professionals interested in this emerging research area!",
    "snippet": "Just finished reading this fascinating paper exploring the intersection of game theory and LLMs - the bidirectional relationship between these two fields opens exciting new research avenues! What I found intriguing most…"
  },
  {
    "slug": "genai-human-learning",
    "title": "Yan et al. (2024)",
    "source_title": "Promises and challenges of generative artificial intelligence for human learning",
    "source_url": "https://www.nature.com/articles/s41562-024-02004-5",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_generativeai-edtech-aiineducation-activity-7413096781837717504-1zNv/",
    "keywords": [
      "GenerativeAI",
      "EdTech",
      "AIinEducation",
      "LearningSciences",
      "EducationalTechnology",
      "AILiteracy",
      "CriticalThinking",
      "FutureOfLearning"
    ],
    "body": "Recently finished reading \"Promises and challenges of generative artificial intelligence for human learning\" by Yan et al..\n\nIt's a great piece that sparks crucial reflections on how GenAI reshapes education and more important, learning.\n\nIn this paper, the authors examine GenAI integration through learning sciences, educational technology, and human-computer interaction lenses, and identify where promises and challenges are, as well as needs that must be addressed for an effective learning. \n\nWhat's crucial, to me, is their acknowledgement that realising these promises depends entirely on how GenAI interacts with learners and educators.\n\nI appreciate authors' concern on the over-reliance use of GenAI. When students perceive AI as omniscient and neutral (which is, unsurprisingly, neither true), it creates dangerous \"illusions\" of knowledge and competence. This over-reliance, inevitably, threatens their critical thinking, creativity, and more important, agency.\n\nThe 'performance paradox' authors highlighted is intriguing. Students achieve better outcomes with GenAI assistance, but removing that support reveals they haven't developed essential skills. This, I think, forces us to reconsider what we're assessing: should we assess how students complete tasks with GenAI assistance, rather than just the outcomes?\n\nDespite GenAI's capabilities, educators' roles evolve rather than disappear: from content production to critical monitoring; from knowledge dissemination to mentorship. The authors note aptly that educators' expertise, which I agree with, remains crucial for ensuring accuracy, relevance, and pedagogical soundness. GenAI, of course, can't replace the human eduators' element of questioning, challenging, and guiding students through uncertainty together.\n\nThe authors highlighted that we need robust standards for evaluating GenAI's effects on learning. This, to me, suggests shifting from 'outcome-focused' to 'process-focused' assessments, reminding me of good old coding interviews where interviewers prioritise problem-solving approaches over final solutions.\n\nThis insightful piece provides crucial foundation for GenAI-learning research. The SCAN framework Alina and I developed (https://lnkd.in/eq646gTB) offers a plausible solution to several questions raised in this paper, particularly on treating GenAI as a learning support - how educators assign tasks to students, or when students prefer self-regulated learning and decide to seek assistance from more knowledgeable others (e.g., educators, GenAI).\n\nMany thanks to Lixiang (Jimmie) Yan, Samuel Greiff, Ziwen Teuber, and Dragan Gasevic for this thought-provoking work.",
    "snippet": "Recently finished reading \"Promises and challenges of generative artificial intelligence for human learning\" by Yan et al.. It's a great piece that sparks crucial reflections on how GenAI reshapes education and more…"
  },
  {
    "slug": "genai-product-safety-standard",
    "title": "Department of Education (2026)",
    "source_title": "Guidance for Generative AI: product safety standards",
    "source_url": "https://www.gov.uk/government/publications/generative-ai-product-safety-standards/generative-ai-product-safety-standards",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_generativeai-edtech-aiineducation-activity-7421431760351019008-cVr9/",
    "keywords": [
      "GenerativeAI",
      "EdTech",
      "AIinEducation",
      "EducationalTechnology",
      "AIEthics",
      "LearningDesign",
      "CognitiveScience",
      "AIGovernance"
    ],
    "body": "Just finished reading 'Guidance for Generative AI: product safety standards' published by the Department for Education last week. \n\nI appreciate this document addresses several critical dimensions about GenAI in education: cognitive development, emotional/social development, mental health, and manipulation.\n\nIn the cognitive development section, I appreciate the highlight of the 'friction by design' principle. The guidance suggests prompting learners for input before providing answers, tracking cognitive offloading, and maintaining process-focused learning. I wonder: could developers create tools that let educators calibrate difficulty levels based on individual student capability? This, indeed, preserves educator agency while leveraging AI.\n\nTo me, the behavioural science opportunities here are rich: preventing cognitive offloading, and building metacognitive skills achieve similar goals through behavioural interventions (the SCAN framework that Alina and I developed offers a great basis; https://lnkd.in/eanDnGbm). I suspect detection methods could include response speed and cursor movement patterns (similar to authenticity protocols from Gorilla Experiment Builder and Prolific).\n\nTracking cognitive offloading is, of course, intriguing. However, implementation questions remain: How do we make educational AI compelling enough that students choose it over tools that enable offloading in the long term (reminding me of Tris' fascinating presentation on 'veracity offloading'; https://lnkd.in/e8HJg--k)? I suspect social proof and gamification are potential solutions.\n\nThe emotional development section's emphasis on psychological safety and preventing emotional dependence is great. Yet - does implementation requires genuine educator consultation beforehand? What monitoring autonomy do teachers need? What actually works in their daily practice?\n\nRegarding the mental health and manipulation sections, my concerns with these guidelines are that they sound excellent, but recent empirical research shows AI sycophancy increases over extended conversations. Hence, how do we prevent these safeguards from degrading as student-AI interactions continue?\n\nWhile the guidance provides a thoughtful framework, implementation is, I think, going to require a deep collaboration between developers, educators, and researchers. For instance:\n\n(1) What facilitates compatibility between the triangular relationship (teachers, students, and GenAI)?\n\n(2) Do we need to have more clarity on learning outcomes, i.e. what students genuinely need to learn vs. what they can automate?\n\n(3) How do teachers and GenAI share knowledge delivery (thoughtful learning design)?",
    "snippet": "Just finished reading 'Guidance for Generative AI: product safety standards' published by the Department for Education last week. I appreciate this document addresses several critical dimensions about GenAI in…"
  },
  {
    "slug": "gpt4-persuasiveness",
    "title": "Salvi et al. (2025)",
    "source_title": "On the conversational persuasiveness of GPT-4",
    "source_url": "https://doi.org/10.1038/s41562-025-02194-6",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_on-the-conversational-persuasiveness-of-gpt-activity-7337800187710464002-vdbw",
    "keywords": [
      "AI",
      "ArtificialIntelligence",
      "Persuasion",
      "MachineLearning",
      "DigitalEthics",
      "HumanAIInteraction",
      "TechResearch",
      "AIEthics",
      "MachineLearning",
      "DigitalPersuasion",
      "LLM",
      "HumanAIInteraction",
      "InformationLiteracy"
    ],
    "body": "Can GenAI become more persuasive than humans in everyday conversations? And what does this mean for our digital future?\n\n\nI recently read a research from Nature Human Behaviour on \"On the conversational persuasiveness of GPT-4\" by Francesco Salvi, Dr. Manoel Horta Ribeiro, Riccardo Gallotti, and Dr. Robert West that dives deep into this hot topic. \n\n\nWhat I found intriguing was how different an arguement is presented between human and GenAI. With the textual analysis, authors found that OpenAI's #GPT4 heavily relied on logical reasoning and factual knowledge, while humans displayed more appeals to similarity, expressions of support, and employed more storytelling. This reminds me of how Mr. Spock interacted with his crew in the Star Trek movie.\n\n\nAnother fascinating finding is when a human participant was asked to recognize whether the opponent is a human or an GenAI in a debate. Participants correctly identified AI opponents about 75% of the time, suggesting GPT-4's writing style has distinctive features. However, when debating other humans, identification success was no better than random chance! \n\n\nEven more interesting, when participants believed they were debating a GenAI, they became more agreeable compared to when they thought they were debating humans.\n\n\nI did a quick test with ChatGPT 4.5 and 4.1 based on an example of a Human-AI (personalized) debate. I noticed these models still demonstrate highly logical and analytical thinking. It would be interesting, as the authors suggest, to conduct experiments with other LLMs such as Anthropic's #Claude and Google's #Gemini, with prompts that instruct LLMs to rely less on logical reasoning and showcase more appeals to support and trust.\n\n\nAs for everyday users, I think that:\n\n  1. The implications are worth considering. As authors note: \"Malicious actors interested in deploying chatbots for large-scale disinformation campaigns could leverage fine-grained digital traces and behavioural data, building sophisticated, persuasive machines capable of adapting to individual targets.\"\n\n\n  2. The debate structure (the opening–rebuttal–conclusion structure), which authors note it is based on a simplified version of the format commonly used in competitive academic debates, is valuable for structuring LLM conversations.\n\n\n  3. This research raises some deeper questions about persuasion itself: What kind of conversation style is truly more persuasive? And how concerning is it that these persuasive effects were achieved with minimal personal information and simple prompting techniques?\n\n\nMany thanks to the authors for this illuminating research that pushes us to think more critically about concerns around personalization and GenAI persuasion.",
    "snippet": "Can GenAI become more persuasive than humans in everyday conversations? And what does this mean for our digital future? I recently read a research from Nature Human Behaviour on \"On the conversational persuasiveness of…"
  },
  {
    "slug": "hai-augmentation-meta-analysis-2024",
    "title": "Vaccaro et al. (2024)",
    "source_title": "When combinations of humans and AI are useful: A systematic review and meta-analysis",
    "source_url": "https://doi.org/10.1038/s41562-024-02024-1",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_when-combinations-of-humans-and-ai-are-useful-activity-7393717585168740352-6XNQ",
    "keywords": [
      "HumanAICollaboration",
      "AIResearch",
      "MetaAnalysis",
      "AugmentedIntelligence",
      "DecisionMaking",
      "CreativeTasks",
      "SystematicReview",
      "FutureOfWork"
    ],
    "body": "Just finished reading \"When combinations of humans and AI are useful: A systematic review and meta-analysis\" by Michelle Vaccaro, Abdullah Almaatouq and Thomas Malone.\n\n\nIn this study they are interested in answering a question: when are the combinations of humans and AI truly useful? This is one of my key interests in Human-GenAI interactions.\n\n\nThis analysis examined 370 unique effect sizes from 106 experiments published between 2020-2023, measuring both 'human-AI synergy' (human-AI performing better than both human alone and AI alone) and 'human augmentation' (human-AI performing better than humans alone).\n\n\nWhen interpreting these results, it's crucial to note their inclusion criteria such as the analysis required studies reporting performance of humans alone, AI alone, and human-AI systems. This excludes tasks that might be impossible for either to perform independently.\n\n\nThey found that, on average, human-AI systems performed worse than the best of either humans or AI alone (lacking 'synergy'). These systems, however, did consistently outperform humans working independently ('human augmentation').\n\n\nApart from that, they found that task type matters: Human-AI combinations showed negative synergy for decision tasks but positive synergy for creation tasks. \n\n\nWhat I also found intriguing from their findings is that when humans outperformed AI alone, the combined system outperformed both. However, when AI outperformed humans alone, combining them reduced performance compared to AI alone.\n\n\nAs we design future human-AI systems, this research suggests focusing less on confidence levels or explanations (which surprisingly didn't significantly affect performance) and more on understanding how task types and relative performance influence outcomes.\n\n\nI especially appreciated their call for developing \"commensurability criteria\" under \"A roadmap for future work: finding human–AI synergy\" to facilitate systematic comparisons across studies as we continue this important research.\n\n\nFor those who are interested in reading this paper, I highly recommend reading thoroughly the Discussion section (including the Limitations and \"A roadmap for future work: finding human–AI synergy\"). I also recommend reading supplmentary functions for definitions of key terms such as 'Human-AI Synergy', 'Human Augmentation', 'AI Augmentation' and 'Negative Human-AI Synergy'.",
    "snippet": "Just finished reading \"When combinations of humans and AI are useful: A systematic review and meta-analysis\" by Michelle Vaccaro, Abdullah Almaatouq and Thomas Malone. In this study they are interested in answering a…"
  },
  {
    "slug": "how-ai-impacts-skill-formation",
    "title": "Shen & Tamkin (2026)",
    "source_title": "How AI Impacts Skill Formation",
    "source_url": "https://doi.org/10.48550/arXiv.2601.20245",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_how-ai-impacts-skill-formation-activity-7424330862713806848-eVHB/",
    "keywords": [
      "ArtificialIntelligence",
      "SkillDevelopment",
      "HumanAICollaboration",
      "CognitiveScience",
      "LifelongLearning",
      "FutureOfWork",
      "AIAugmentation",
      "MetacognitiveAI"
    ],
    "body": "Just finished reading a preprint \"How AI Impacts Skill Formation\" by Judy Hanwen Shen and Alex Tamkin from Anthropic.\n\nAs we pursue human-AI augmentation and productivity gains, perhaps we're overlooking a critical question: what happens to skill acquisition, retention, or decay over time?\n\nIn this empirical, mixed-methods study, they conduct randomised experiments to study how developers gained mastery of a new asynchronous programming library with and without AI assistance. \n\nTheir main finding is that developers using AI assistance to learn a new programming library scored 17% lower on skill assessments compared to those learning without AI - even though AI didn't significantly speed up task completion. Participants in treatment group felt 'lazy' and reported 'gaps in understanding' afterward, which is an indicator of cognitive offloading (works of Prof. Dr. Michael Gerlich).\n\nThis connects to Macnamara et al.'s on cognitive skill decay in GenAI era I reviewed. The question I keep returning to is what Tris calls 'veracity offloading': how does such cognitive delegation compound across expertise levels over time? \n\nIt's also the \"Iron Man paradox\" question I emphasised in webinar: \"What do you do when you're without the armor?\"\n\nWhat's also intriguing is six distinct AI interaction patterns researchers identified. High scorers asked conceptual questions or requested explanations alongside code, while low scorers simply delegated to AI without engagement. This mirrors the task identification dynamics in the SCAN framework that Alina and I developed: whether users identify tasks as Substitute (automation) versus Aid/Complement (augmentation/critical engagement).\n\nThe debugging skills gap was significant. As researchers noted: if workers' skill formation is inhibited by AI assistance, they may lack the necessary skills to validate and debug AI-generated code. This, I think, exemplifies the upskilling-deskilling paradox we emphasised in SCAN: tasks oscillating between Aid and Complement subzones over time.\n\nAs researchers noted, we've historically moved from 'producer' to 'supervisor'. In the GenAI era, however, how does a person become a competent supervisor without being a producer first - the very role GenAI now occupies?\n\nWhen considering AI as 'a scaffold', perhaps metacognitive prompting framework (like CIA framework Shantanu and I developed) could help reduce automation bias and illusion of understanding?\n\nThe study was rigorously controlled (impressive pilot work addressing non-compliance and confounding variables), but it's a one-off snapshot. What we  need, I suspect, is longitudinal studies tracking these dynamics over months and years. \n\nMany thanks to the researchers for this timely work. I'm looking forward to seeing how this research direction evolves.",
    "snippet": "Just finished reading a preprint \"How AI Impacts Skill Formation\" by Judy Hanwen Shen and Alex Tamkin from Anthropic. As we pursue human-AI augmentation and productivity gains, perhaps we're overlooking a critical…"
  },
  {
    "slug": "human-generated-datasets-for-ai-safety-fine-tuning",
    "title": "Mustafa and Wu (2025)",
    "source_title": "Human-Generated Datasets for AI safety Fine-Tuning",
    "source_url": "https://www.linkedin.com/feed/update/urn:li:activity:7328089036219191299/",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_aipolicy-artificialintelligence-techpolicy-activity-7345068218107015168-7l77",
    "keywords": [
      "AIEthics",
      "MachineLearning",
      "DataQuality",
      "AITraining",
      "LanguageModels",
      "AILocalization",
      "ResponsibleAI",
      "AIBiasReduction"
    ],
    "body": "Just finished reading a report on human-generated datasets for AI safety fine-tuning. It highlights how modern language models often struggle with high-risk topics and cultural nuances, especially in low-resource languages.\n\n\nThis report has broadened my horizon on one of the fundamental challenges with GenAI: data quality. After all, garbage in, garbage out. It presents a compelling case for human-generated datasets created by domain experts with both subject-matter expertise and cultural-linguistic fluency. Their 7-step framework and prioritization matrix provide practical guidance for organizations looking to improve AI performance while reducing content-related risks.\n\n\nWhat I found interesting in the report was the comparison between 'synthetic' and 'human-generated' data. While synthetic data offers scale, it tends to reinforce existing biases, and often lacks real-world context in culturally sensitive areas - issues that human-generated data can mitigate.\n\n\nI'm also intrigued by the tangible benefits highlighted when using fine-tuning LLMs with human-generated data, such as improved accuracy, reduced bias, and enhanced cultural adaptability through localization efforts.\n\n\nAfter reading this report, I'm interested in exploring the pros and cons of these two data types, and possibly, how they might complement each other, which creates a hybrid approach that leverages the strengths of both.\n\n\nMany thanks to the authors Alisar Mustafa and Cherry Wu from Duco for this insightful report. Looking forward to diving deeper into this critical area of responsible AI development.",
    "snippet": "Just finished reading a report on human-generated datasets for AI safety fine-tuning. It highlights how modern language models often struggle with high-risk topics and cultural nuances, especially in low-resource…"
  },
  {
    "slug": "human-machine-extended-organisms",
    "title": "Hamilton and Benjamin (2019)",
    "source_title": "The Human-Machine Extended Organism",
    "source_url": "https://psycnet.apa.org/doi/10.1016/j.jarmac.2019.01.001",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_the-human-machine-extended-organism-activity-7383890055360126976-KPNq",
    "keywords": [
      "HumanAICollaboration",
      "CognitiveScience",
      "DigitalEcology",
      "ExtendedCognition",
      "GenAI",
      "CognitiveOffloading",
      "FutureOfWork",
      "TechEthics"
    ],
    "body": "Just finished reading \"The Human-Machine Extended Organism\" by Kristy Hamilton and Aaron Benjamin. \n\n\nWhile the paper focuses on human-internet interaction, many arguments apply perfectly between human and GenAI.\n\n\nIn this paper, the researcher examine the relationship between human and machine (the Internet), and reframe it as an extended organism rather than two separate systems. \n\n\nI appreciate their frame of an \"integrative system of internal and external cognitive processes\". They note: \"Much of what we think of as human memory in a digital ecology is the product of an integrative system selected to meet the demands of a particular cognitive task. The ability to effectively integrate internal and external processes is the critical feature of a successful cognitive agent.\" \n\n\nI kept nodding through the section on \"Consequences for Outsourcing Retrieval\", which are related to explicitly developing an organized system and how expertise are built. For instance, the researchers noted: \"One cannot become an expert birder by having a hard drive full of bird photographs. Generalization comes from internalized knowledge.\" This section, to me, highlights several critical aspects of human-GenAI interaction.\n\n\nI appreciate what researchers mentioned towards the end:\n\n\n\"People need to decide how to share cognitive responsibilities with a machine partner that is very different from themselves. They need to do so in a way that maximizes the benefits of the partnership and minimizes the risks. Doing so requires a deep understanding of what humans are good at, what machines are good at, and how to ensure access to relevant information in the conditions in which it is likely to be needed.\"\n\n\nThis raises a question we often overlook before completing a task with GenAI: what is my role and responsibility as the human user? What role and responsibilities should I assign to GenAI?\n\n\nMany thanks to the researchers for this interesting paper.",
    "snippet": "Just finished reading \"The Human-Machine Extended Organism\" by Kristy Hamilton and Aaron Benjamin. While the paper focuses on human-internet interaction, many arguments apply perfectly between human and GenAI. In this…"
  },
  {
    "slug": "llm-agents-cooperate-social-dilemma-simulation",
    "title": "Willis et al. (2025)",
    "source_title": "Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma",
    "source_url": "https://doi.org/10.48550/arXiv.2501.16173",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_llm-agents-cooperate-iterated-prisoners-dilemma-activity-7345793644504707073-emb6",
    "keywords": [
      "AICooperation",
      "GameTheory",
      "LLMAgents",
      "MultiAgentSystems",
      "EvolutionaryComputing",
      "ArtificialIntelligence",
      "AIStrategy",
      "MachineLearning"
    ],
    "body": "I recently explored this fascinating paper investigating how LLM agents behave in strategic interactions - whether they cooperate or compete with each other in social dilemmas.\n\n\nWhat makes this research distinctive from other related research is their novel approach: rather than having LLMs output individual actions, the researchers prompted models (OpenAI's #ChatGPT4o and Anthropic's #Claude 3.5 Sonnet) to generate 'complete' strategies for iterated Prisoner's Dilemma. \n\n\nThe researchers investigated whether LLM agents perform better when prompted with 'aggressive' (and thus compete), 'cooperative' (and thus cooperate), or 'neutral' attitudes. These strategies were then converted to Python algorithms, and evaluated in tournaments with selection pressure favoring higher-performing strategies.\n\n\nTwo findings particularly resonated with me:\n\n\nFirst, the strategies produced by all three prompting approaches were inherently game-theoretic in nature. Even when the task was obfuscated (in the 'Prose' prompt), the models recognized that game theory principles applied. This suggests generative agents will reason appropriately about strategic scenarios in real-world applications.\n\n\nSecond, different LLMs exhibited distinct biases affecting the success of aggressive versus cooperative strategies, which highlight the importance of model selections.\n\n\nFor future work, I wonder how reasoning vs. non-reasoning models (both open-source and close, of course) might perform differently. Would assigning human/machine roles affect strategy development? How might changing payoff structures (something I investigated in my thesis) influence LLM responses, and thus, performance?\n\n\nMany thanks to researchers Richard Willis and Dr. Yali Du from King's College London, Dr. Joel Leibo from Google and Professor Michael Luck from University of Sussex for this amazing study!\n\n\nAs someone who studied similar contexts in the PhD thesis, I'm excited about the intersections of game theory, LLMs, and multi-agent systems that this work opens up. I'm happy to connect with fellow enthusiasts, academics, and professionals interested in this research area!",
    "snippet": "I recently explored this fascinating paper investigating how LLM agents behave in strategic interactions - whether they cooperate or compete with each other in social dilemmas. What makes this research distinctive from…"
  },
  {
    "slug": "llm-pollution-online-behavioral-research",
    "title": "Rilla et al. (2025)",
    "source_title": "Recognising, Anticipating, and Mitigating LLM Pollution of Online Behavioural Research",
    "source_url": "https://doi.org/10.48550/arXiv.2508.01390",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_recognising-anticipating-and-mitigating-activity-7359968413454139392-KSor",
    "keywords": [
      "ResearchMethodology",
      "AIEthics",
      "SocialScience",
      "LLMs",
      "OnlineResearch",
      "ResearchValidity",
      "AIBias",
      "FutureOfResearch"
    ],
    "body": "I recently read \"Recognising, Anticipating, and Mitigating LLM Pollution of Online Behavioural Research\" that introduces the concept of 'LLM Pollution'.\n\n\nThis is an emerging threat to online behavioural research where GenAI become involved in online tasks designed to measure human responses.\n\n\nThe authors identify three variants of this phenomenon, but I found two particularly concerning for social science research:\n\n\nIn the 'Full LLM Delegation', participants outsource entire studies to AI tools or agents. As GenAI agents become more prevalent these days (imagine calling multiple agents to participate in several studies simultaneously), researchers face homogeneous responses that no longer reflect true, natural human cognitive variation. Also, these models are trained on data with potential human biases, amplified with additional biases they themselves introduce. Thus, this combination of biases can, as one imagines, produce unrealistic results that complicate research findings and any policies based on them.\n\n\nThe 'LLM Spillover' variant is, in my opinion, equally troubling. In the experiment, participants may alter their behavior simply in anticipation of LLM involvement, even when none exists. I agree with the authors that this can lead to unintended consequences, such as participants reducing their effort with the rationalisation that 'everyone is cheating with LLMs anyway' - a form of moral licensing behaviour.\n\n\nWhile recognizing the consequences of LLM Pollution in online behavioural research, the paper offers some practical mitigation strategies, which are worth exploring. \n\n\nIn addition, online participant recruitment platforms like Prolific now provide guideline on 'Authenticity Check' (https://lnkd.in/eAJz55fy) to help verify human participation.\n\n\nI shall end this post by quoting from the paper: \n\n\n'As LLMs become increasingly embedded in everyday life, their use in cognitive, communicative, and problem-solving tasks may no longer be an exception, but the norm. This raises a more fundamental question: at what point does LLM-assisted behaviour cease to be “pollution” and instead become part of the ecological baseline we must account for? While mitigation remains essential for preserving the integrity of current methods, the long-term challenge may lie in adapting our theoretical frameworks to a world where human reasoning is increasingly shaped by intelligent machines.'\n\n\nMany thanks to Raluca Rilla, Tobias Werner, Hiromu Yakura, Iyad Rahwan, and Anne-Marie Nussberger from Max Planck Institute for Human Development for this timely contribution.",
    "snippet": "I recently read \"Recognising, Anticipating, and Mitigating LLM Pollution of Online Behavioural Research\" that introduces the concept of 'LLM Pollution'. This is an emerging threat to online behavioural research where…"
  },
  {
    "slug": "llm-sycophancy",
    "title": "Malmqvist (2024)",
    "source_title": "Sycophancy in Large Language Models: Causes and Mitigations",
    "source_url": "https://doi.org/10.48550/arXiv.2411.15287",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_sycophancy-in-llms-causes-and-mitigations-activity-7335619551155380224-7we8",
    "keywords": [
      "AILiteracy",
      "GenerativeAI",
      "MachineLearning",
      "LLMs",
      "AIEthics",
      "AIAlignment",
      "AIBias",
      "TechTrends"
    ],
    "body": "Are your GenAI assistants telling you what you want to hear, rather than what you need to know?\n\n\nI just finished reading \"Sycophancy in Large Language Models: Causes and Mitigations\" by Dr. Lars Malmqvist (Partner at Implement Consulting Group). This paper provides a technical survey of sycophancy in LLMs, synthesizing recent research on its causes, impacts, and potential mitigation strategies - an excellent introduction to a critical issue in GenAI development. \n\n\nSycophancy, as the author defined is, the tendency of LLMs to exhibit \"excessively agreeing with or flattering users\" behavior - essentially telling us what we want to hear, rather than what's accurate. \n\n\nThis poses significant risks: it reinforces our confirmation bias on familiar topics, and can easily manipulate us on unfamiliar subjects where we rely on generative AI for information.\n\n\nWhat struck me most while reading was the section on the challenges in defining alignment. The alignment problem - ensuring generative AI systems behave in accordance with human values and intentions - is fundamental to addressing sycophancy. When we struggle to precisely define concepts like 'truthfulness' and 'helpfulness', we inadvertently create systems that prioritize user agreement over factual accuracy.\n\n\nAs for everyday users, we can mitigate this issue by:\n\n1. Increasing our generative AI literacy through reading papers like this, and sharing experiences with others, and taking relevant online courses.\n\n\n2. When interacting with generative AI, modify our prompts to play the Devil's Advocate, or show multiple perspectives (as if having an Angel and a Devil on our shoulders). An example prompt is: \n\n\"Analyze this topic from multiple viewpoints, including ones that challenge my perspective.\"\n\n\n3. Comparing responses across different generative AI models to identify subtle sycophantic tendencies\n\n\nMany thanks to Dr. Lars Malmqvist for this insightful paper that helps us think more critically about human-AI interaction, and reminds us to maintain healthy skepticism when working with these powerful tools.",
    "snippet": "Are your GenAI assistants telling you what you want to hear, rather than what you need to know? I just finished reading \"Sycophancy in Large Language Models: Causes and Mitigations\" by Dr. Lars Malmqvist (Partner at…"
  },
  {
    "slug": "long-term-cognitive-cost-ai",
    "title": "Inie (2025)",
    "source_title": "The Cognitive Cost of Generative AI Mapping long-term risks and moderating factors",
    "source_url": "https://www.nannainie.com/_files/ugd/cf986a_96612c9ab2bb4864be2bbbf3b73f416b.pdf",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_new-position-paper-the-cognitive-cost-of-activity-7365640920718876673-JoVx",
    "keywords": [
      "GenerativeAI",
      "CognitiveLoad",
      "FutureOfWork",
      "AIResearch",
      "TechnoStress",
      "KnowledgeWork",
      "AIEthics",
      "CognitiveScience"
    ],
    "body": "Just finished reading this thought-provoking paper 'The Cognitive Cost of Generative AI Mapping long-term risks and moderating factors' by Nanna Inie (Assistant Professor at the IT-Universitetet i København).\n\n\nAs an AI Behavioural Researcher, honestly, I am eager to see this research flourish in the coming years (or months, perhaps). \n\n\nThe cognitive cost of GenAI adoption has been largely overlooked by corporations rushing to adopt these technologies. This, to me, is one of the few potential explantions why many GenAI implementations fail in business settings.\n\n\nInie frames GenAI use among knowledge workers as 'a double-edged sword', which does remind me of the calculator analogy or the Google effect (which may be intensifying as search engines incorporate AI).\n\n\n\"This (GenAI adoption) does sound great, but what is 𝘵𝘩𝘦 𝘤𝘢𝘵𝘤𝘩?\"\n\n\nI believe that we're still discovering what harms GenAI might cause, especially regarding long-term cognitive costs. This, of course, requires ongoing monitoring and research, particularly as AI agents and agentic AI amplify these concerns.\n\n\nThe paper connects negative feelings during challenging cognitive tasks with long-term cognitive benefits, which, to me, is similar to how physical training, despite discomfort, prevents chronic diseases later in life. Does this highlight the essence of carefully positioning ourselves alongside GenAI, in defining clearer roles, tasks, and thus, expectations. These, I believe, are some of crucial elements for building a foundation where trust in human-GenAI relationship can be built upon.\n\n\nThe reduced sense of agency when using GenAI creates an interesting feedback loop: those who are confident in their abilities, of course, maintain power over the final output, while those less confident may surrender more control to AI systems. \n\n\nThis requires constant self-evaluation, or an introspection, of our knowledge and strategic decisions about what roles we assign to GenAI.\n\n\nViewing knowledge work as a service journey, perhaps, helps identify where GenAI implementation offers benefits versus potential long-term costs. A concerning outcome might be increasingly homogeneous quality of work products as people over-rely on GenAI output, as time goes on.\n\n\nInie's proposed research areas such as GenAI's influence on social relationships, impact on metacognitive evaluation, and technostress deserve serious attention from behavioural and cognitive scientists.\n\n\nTrained as an economist, I'm naturally drawn to finding an 'equilibrium' between optimal human cognitive load and GenAI assistance. Perhaps, perhaps, perhaps - by starting to put weight of cognitive costs on against the benefits where GenAI adoption has been emphasising on, may we discover this balance in the long run.\n\n\nMany thanks to the author for this timely contribution!",
    "snippet": "Just finished reading this thought-provoking paper 'The Cognitive Cost of Generative AI Mapping long-term risks and moderating factors' by Nanna Inie (Assistant Professor at the IT-Universitetet i København). As an AI…"
  },
  {
    "slug": "narrow-search-effect",
    "title": "Leung and Urminsky (2025)",
    "source_title": "The narrow search effect and how broadening search promotes belief updating",
    "source_url": "https://www.pnas.org/doi/10.1073/pnas.2408175122",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_narrow-search-effect-on-search-belief-updating-activity-7384157191920054273-vUPz",
    "keywords": [
      "AILiteracy",
      "ConfirmationBias",
      "PromptEngineering",
      "InformationSeeking",
      "BeliefsUpdating",
      "GenerativeAI",
      "HumanAIInteraction",
      "CognitiveScience"
    ],
    "body": "Just finished reading \"The narrow search effect and how broadening search promotes belief updating\" by Eugina Leung and Oleg Urminsky.\n\n\nIt is fascinating paper about how people search for information online, and is timely as we navigate interactions with GenAI platforms for search and advice.\n\n\nIn this paper, they raise several important questions: the balance between 'breadth' and 'depth' in information search, and how search algorithms should be designed to promote belief updating.\n\n\nThe researchers demonstrate that, even without algorithmic bias, 'echo chambers' persist as we naturally use 'directionally narrow search terms' that reflect our existing beliefs. This 'narrow search effect' reinforces confirmation bias across Google, ChatGPT, and other platforms.\n\n\nWhat I find intriguing was their finding across 21 studies: when information technology provides broader information (beyond what was specifically requested), people update their beliefs more after searching. \n\n\nIn the case of interacting GenAI platforms like OpenAI's ChatGPT 3.5 explicitly acknowledging opposing viewpoints, users assigned to directionally narrow queries showed significant bias in their post-search beliefs.\n\n\nThe paper suggests structural changes to search and AI algorithms can mitigate confirmation bias. While LLM's sycophantic nature in conversational AI platforms like Anthropic's #Claude, xAI's Grok, and Google's #Gemini can, of course, reinforce user's prior beliefs, encouraging dialogical, multi-turn conversations might help user explore broader perspectives, thus allow for more reflection and belief updating compared to single-query Google searches.\n\n\nAs we develop next-generation AI systems, the researchers highlight the need for research more fully tests psychologically informed prompt-engineering approaches, an emerging question that bridges the psychology of decision-making and human–computer interaction (reminding me of related conversations I've had with Tris).\n\n\nAfter reading, I wonder:\n\n1. How do these findings apply to domains like trading, mental health support, and product recommendations?\n\n2. How might AI agents amplify these issues? \n\n3. When combined with research on automation bias, what behavioural interventions could mitigate the combination of human confirmation bias and LLM sycophancy?\n\n\nThanks to the researchers for this insightful work that bridges psychology and human-computer interaction!",
    "snippet": "Just finished reading \"The narrow search effect and how broadening search promotes belief updating\" by Eugina Leung and Oleg Urminsky. It is fascinating paper about how people search for information online, and is…"
  },
  {
    "slug": "nudging-ai-system",
    "title": "Ganapini et al. (2023)",
    "source_title": "Value-based Fast and Slow AI Nudging",
    "source_url": "https://doi.org/10.48550/arXiv.2307.07628",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_value-based-fast-and-slow-ai-nudging-activity-7384540154436419586-w14x",
    "keywords": [
      "BehaviouralScience",
      "ArtificialIntelligence",
      "GenAI",
      "DecisionMaking",
      "Nudging",
      "HumanAICollaboration",
      "CognitiveScience",
      "AIEthics"
    ],
    "body": "Just finished reading \"Value-based Fast and Slow AI Nudging\" by Ganapini et al., which explores the intersection between behavioural science nudges and GenAI.\n\n\nThis is another must-read for behavioural scientists, and anyone interested in exploring how behavioural insights can enhance AI-assisted decision-making.\n\n\nIn this paper, the researchers propose a value-based AI-human collaborative framework called 'Fast and Slow Collaborative AI' (FASCAI), where AI systems nudge humans by providing decision recommendations. \n\n\nIn FASCAI, the machine strategically chooses how to interact with human decision-makers. Given a problem, the AI generates recommendations presented in ways that stimulate appropriate thinking modalities: \n\n- System 1 (fast, automatic thinking),\n\n- System 2 (slow, deliberate reasoning), or \n\n- Metacognition (reflecting on one's own thought processes).\n\n\nI appreciate the researchers' introduction of 'metacognitive nudges' as a third type of nudges, which extends Cass Sunstein's conversation about two macrocategories of nudges in his book \"Human Agency and Behavioral Economics\". As they noted: \"prompting metacognition is a way to make us introspect on possible gaps in our knowledge or lack of confidence in reaching a decision.\"\n\n\nA fascinating, key aspect of FASCAI is selecting which nudging mechanism to employ based on values relevant to the decision environment, such as human upskilling, human agency, and decision quality. \n\n\nTheir concept of 'five human-machine decision modalities' about cognitive load and autonomy for humans is intriguing to me. Three adopt nudging mechanisms from machine to human (engaging System 1, System 2, or Metacognition), while in the other two, either machine or human decides autonomously.\n\n\nFor those interested, I recommend studying the sections on \"How to Implement Nudges\", \"When to Employ Each Nudging Modality: A Value-Based Approach\", and \"Research Questions\". \n\n\nThis framework complements their SOFAI architecture mentioned in my previous post (https://lnkd.in/eeYebmrG), which Michael Hallsworth, PhD, Elisabeth Costa, and Deelan Maru from BIT highlighted their Augment paper as a potential way forward for improving AI with behavioural science (https://lnkd.in/ekPApJum). \n\n\nMany thanks to Marianna B. Ganapini, Francesco Fabiano, Lior Horesh, Andrea Loreggia, Nicholas Mattei, Keerthiram Murugesan, Vishal Pallagani, Francesca Rossi, Biplav Srivastava, and K. Brent Venable for a thought-provoking work!\n\n\nI look forward to seeing and contributing to how this research evolves, as we see growing importance of cognitive aspects in human-AI interaction.",
    "snippet": "Just finished reading \"Value-based Fast and Slow AI Nudging\" by Ganapini et al., which explores the intersection between behavioural science nudges and GenAI. This is another must-read for behavioural scientists, and…"
  },
  {
    "slug": "predict-soc-sci-expt-results-with-llm",
    "title": "Hewitt et al. (2024)",
    "source_title": "Predicting Results of Social Science Experiments Using Large Language Models",
    "source_url": "https://samim.io/dl/Predicting%20results%20of%20social%20science%20experiments%20using%20large%20language%20models.pdf",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_predict-social-science-experiment-results-activity-7378654840673161216-bJ8N",
    "keywords": [
      "AIResearch",
      "SocialScience",
      "MachineLearning",
      "ResearchMethodology",
      "SurveyExperiments",
      "LLM",
      "DataScience",
      "ExperimentalDesign"
    ],
    "body": "Just finished reading a preprint \"Predicting Results of Social Science Experiments Using Large Language Models\" by Luke Hewitt, Ashwini Ashokkumar, Isaias Ghezae, and Robb Willer. \n\n\nIn this study, the researchers explored whether LLMs could accurately simulate human responses, and thus predict results of representative sample survey experiments in the US.\n\n\nSpecifically, they built an archive of 70 pre-registered, nationally representative survey experiments from the US. They prompted LLMs (e.g. GPT-4) with an introductory message about the study setting, specific demographic profiles randomly drawn from representative samples, experimental stimuli, and outcome questions with response scales. By comparing responses across conditions, they generated LLM-predicted effect sizes to correlate with original experimental effects.\n\n\nThree intriguing findings for me from this study:\n\n1. Current-generation LLMs can accurately predict both size and direction of survey experimental effects, with accuracy improving across model generations. \n\n\n2. The authors found that LLM-derived predictions remained highly accurate even for studies published after the LLMs' training data cutoff dates.\n\n\n3. LLMs are more accurate for survey experiments than field experiments, and, unsurprisingly, better with text-based treatments than non-text treatments. \n\n\nI appreciate the authors notes in the Discussion section about both benefits (cheap and quick pilot studies to identify promising research), and limitations (such as LLMs underestimating variance in human responses) of such method.\n\n\nThe authors suggested that, which I also agree with, an LLM-based tool like this one is a great 'pre-pilot': it can help identify effective interventions before a costly implementation, which thus potentially complements human expert forecasts.\n\n\nAfter reading this, I'm curious about several extensions: \n\n(1) How would results differ across cultural contexts (WEIRD vs. non-WEIRD populations)? \n\n(2) How well would these predictions translate to real-world implementation? \n\n(3) What about testing with open-source LLMs such as OpenAI's gpt-oss? \n\n(4) And when expert predictions diverge from LLM predictions, how should policymakers reconcile these differences?\n\n\nMany thanks to the authors for this eye-opening research that bridges AI capabilities with social science methodology.",
    "snippet": "Just finished reading a preprint \"Predicting Results of Social Science Experiments Using Large Language Models\" by Luke Hewitt, Ashwini Ashokkumar, Isaias Ghezae, and Robb Willer. In this study, the researchers explored…"
  },
  {
    "slug": "psychometric-framework",
    "title": "Serapio-García et al. (2025)",
    "source_title": "A psychometric framework for evaluating and shaping personality traits in large language models",
    "source_url": "https://doi.org/10.1038/s42256-025-01115-6",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_naturemachineintelligence-aisafety-deepmind-activity-7407754378310471680-XrAU",
    "keywords": [
      "ArtificialIntelligence",
      "LargeLanguageModels",
      "Psychometrics",
      "AIResearch",
      "MachineLearning",
      "SyntheticPersonas",
      "ResponsibleAI",
      "ComputationalPsychology"
    ],
    "body": "Just finished reading \"A psychometric framework for evaluating and shaping personality traits in large language models\" by Serapio-García et al..\n\nIt is essential reading for anyone working at the intersection of synthetic personas, AI personality traits, psychometric testing, survey research, and evaluations.\n\nIn this paper, the researchers developed a framework to:\n\n-   quantify personality traits perceived by humans in LLM outputs using psychometric testing,\n\n-   verify whether psychometric tests of LLM personality traits were empirically reliable on a model-by-model basis, and\n\n-   implement mechanisms to systematically shape specific LLM personality traits.\n\nThey applied their construct-validation framework across 18 models (Llama 2, Mistral, Mixtral, and GPT families) with thousands of prompt variations.\n\nTheir findings, interestingly, reveal that personality measurements in LLM outputs under specific prompting configurations are reliable and valid, with stronger evidence for larger and instruction-fine-tuned models. More importantly, they demonstrated that personality in LLM outputs can be shaped to mimic specific human personality profiles.\n\nTwo thought-provoking areas in this paper as I am exploring LLM evaluations and psychometrics:\n\n(1) The evaluation framework\n\nIt is a two-stage approach combining structured prompting (persona instruction, item instruction, and item postamble) with rigorous statistical analysis of reliability and construct validity. Their assessment includes convergent, discriminant, and criterion validity measures.\n\nAdopting this approach with 50 generic self-descriptions from dialogue datasets, they created the social context needed to anchor responses and generate meaningful variation.\n\n(2) Synthetic personality shaping\n\nThey tested both single-trait (one Big Five dimension of personality) and multiple-trait (all five dimensions of Big Five) shaping experiments. Testing 11 models that demonstrated at least 'neutral' reliability, they generated up to 1.125 million social media status updates based on 2,250 simulated human profiles.\n\nThis systematic evaluation framework, I agree, addresses what responsible AI researchers have called for: scientifically evaluating construct validity when studying social-psychological phenomena in AI systems, reminding me of Behavioral AI Institute's 'PsyBench' for evaluating AI's psychological competencies.\n\nFor researchers using AI for developing synthetic personas or interviewing, I highly recommend exploring the supplementary notes for detailed methodology and discussion.\n\nMany thanks to Greg Serapio-García, Mustafa Safdari, Clément CREPY, Luning Sun, Stephen Fitz, Peter Romero, Marwa A., Aleksandra Faust, and Maja Mataric for this insightful work. Looking forward to seeing more research in this space!!",
    "snippet": "Just finished reading \"A psychometric framework for evaluating and shaping personality traits in large language models\" by Serapio-García et al.. It is essential reading for anyone working at the intersection of…"
  },
  {
    "slug": "scan-framework",
    "title": "Tsim and Gutoreva (2025)",
    "source_title": "SCAN: A Decision-Making Framework for Task Assignment with Generative AI",
    "source_url": "https://osf.io/preprints/psyarxiv/g5fd8_v1",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_scan-framework-activity-7412767074328068096-AxZ4/",
    "keywords": [
      "GenerativeAI",
      "HumanAIInteraction",
      "LifelongLearning",
      "FutureOfWork",
      "CognitiveScience",
      "Metacognition",
      "HybridIntelligence",
      "AIDecisionMaking"
    ],
    "body": "Excited to share the work Alina and I have been developing (now available on PsyArXiv) as the first post of 2026!\n\nWe introduce SCAN, a human-centric, decision-making framework that helps learners navigate task assignment with Generative AI effectively. \n\nBuilding on psychology theories such as Vygotsky's Zone of Proximal Development and Metacognition (drawing from Marianna B. Ganapini and her collaborators' fantastic work of building #SOFAI; https://lnkd.in/e-VPYGyn), SCAN addresses a critical challenge in daily task completion: How do we decide when to use GenAI as an assistance at the moment? And more importantly, how do we develop our own capabilities with GenAI in the future?\n\nOur framework systematises AI-human interaction through four sub-zones: \n\n𝗦ubstitute: Tasks GenAI fully handles but human cannot\n𝗖omplement: Where human + AI capabilities combine\n𝗔id: Tasks requiring human lead with AI support as a scaffold\n𝗡on-negotiable: Human-only (e.g., more knowledgeable others) domains\n\nWe demonstrate two applications for knowledge workers and students, showing how to \"scan\" for an effective GenAI use when completing a task. We then discuss how our framework can be related to cognitive offloading, sycophancy, and explore three decision-making modes in Human-GenAI interactions (Automation, Augmentation, Collaboration). Lastly, we examine the future of work: upskilling and deskilling dynamics, and interestingly, the potential \"upskilling-deskilling paradox\".\n\nSCAN, we believe, offers a great starting point before deciding, and debating, whether GenAI complements or replaces human abilities in task completion. We also see it as a plausible decision-making framework that applies to other advanced forms of AI in the future, where human (lifelong) learning is, unsurprisingly, unwaveringly, and undoubtedly, necessary.\n\nGive it a read, and let us know what you think!",
    "snippet": "Excited to share the work Alina and I have been developing (now available on PsyArXiv) as the first post of 2026! We introduce SCAN, a human-centric, decision-making framework that helps learners navigate task…"
  },
  {
    "slug": "shared-identity-ai",
    "title": "Gutoreva (2024)",
    "source_title": "Sharing Identity with AI Systems: A Comprehensive Review",
    "source_url": "https://doi.org/10.1016/j.procs.2023.12.141",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_shared-identity-with-ai-systems-activity-7363072183528484864-cpNL",
    "keywords": [
      "ArtificialIntelligence",
      "DigitalIdentity",
      "HumanAIInteraction",
      "BehavioralScience",
      "AIEthics",
      "CognitiveAugmentation",
      "TechPhilosophy",
      "FutureOfWork"
    ],
    "body": "I recently finished reading an intriguing paper: \"Sharing Identity with AI Systems: A Comprehensive Review\" by Dr. Alina Gutoreva (Assistant Professor in Cognitive Science at the Kazakh-British Technical University). \n\n\nThis is a stimulating work. It argues, and challenges us to consider AI as part of our 'persona' rather than a separate 'entity', which is fundamentally reframing how we conceptualize identity sharing and anthropomorphization, and at a broad level, how we interact with GenAI.\n\n\nIn the paper, Gutoreva explores how different types of identity extend to AI systems, examining methods of connection, applications, and implications across three levels of identity: individual/personal, social, and global. What I found intriguing mostly was how these of our identities might be enhanced through AI interactions.\n\n\nGutoreva's discussion of Synchronous and Asynchronous Methods are both fascinating. The latter reminds me of Agentic AI, where humans delegate periodic tasks to AI agents. In the former, she writes: \"In synchronous methods, such as playing a video game character, the user experiences the digital environment by acting in it by using the character. Similar idea stands with interacting with AI systems.\"\n\n\nThis, to me, raises a fascinating question: If this parallel is valid, what 'character' are we playing when interacting with AI systems?\n\n\nGutoreva's framing of AI as a 'digital prosthesis' that accompanies our digital activity is, also, compelling. This analogy extends beyond AI, where we form 'versions' of ourselves when interacting with others and social media. These constructed selves, often, remain unnoticed until they conflict with our self-perception.\n\n\nSeveral interesting questions I could think of after reading this article:\n\n(1) From a behavioural perspective, how does this AI-integrated self affect our daily lives? \n\n\n(2) Could AI help bridge, with the choice architecture, this gap of dual identities we possess – our current self (based on psychometric and behavioural data, and shared with AI systems), and our potential self we could become?\n\n\n(3) In a social interaction, how might recognizing this relationship empower decision-makers from a strategic perspective? \n\n\n(4) In video games, as Gutoreva exemplified in 'Synchronous Methods', our characters evolve through missions and challenges. How might this parallel our evolution through AI interaction?\n\n\nThese questions, I suspect, merely scratch the surface of the profound implications Gutoreva explores in the shared identity. \n\n\nI'm grateful for her contribution to this emerging field at the intersection of AI, identity, and human behaviour. I eagerly anticipate her future contributions to this field.",
    "snippet": "I recently finished reading an intriguing paper: \"Sharing Identity with AI Systems: A Comprehensive Review\" by Dr. Alina Gutoreva (Assistant Professor in Cognitive Science at the Kazakh-British Technical University).…"
  },
  {
    "slug": "socioaffective-alignment",
    "title": "Kirk et al. (2025)",
    "source_title": "Why Human-AI Relationships Need Socioaffective Alignment",
    "source_url": "https://doi.org/10.1057/s41599-025-04532-5",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_why-humanai-relationships-need-socioaffective-activity-7380900619815133184-iYWv",
    "keywords": [
      "ArtificialIntelligence",
      "AIEthics",
      "HumanAIInteraction",
      "SocialPsychology",
      "AIAlignment",
      "TechnologyAndSociety",
      "GenAI",
      "FutureOfTech"
    ],
    "body": "Just finished reading \"Why Human-AI Relationships Need Socioaffective Alignment\" by Hannah Rose Kirk, Iason Gabriel, Chris Summerfield, Bertie Vidgen and Scott Hale.\n\n\nThis paper is a must-read for anyone interested in the evolving relationship between humans and AI systems, offering valuable insights through the lens of social psychology.\n\n\nThe authors introduce \"socioaffective alignment\": how an AI system behaves within the social and psychological ecosystem co-created with its user, where preferences and perceptions evolve through mutual influence. \n\n\nThe interplay between both macro (sociotechnical) and micro (socioaffective) perspectives within human-GenAI relationships is intriguing, especially as these relationships become increasingly bi-directional rather than unidirectional.\n\n\nDrawing from Computers-are-social-actors theory, media equation theory, and social response theory, the authors explain why we might treat chatbots differently than washing machines or smartphones. This perspective becomes increasingly important as relationships between human principals and AI agents deepen.\n\n\nI appreciate how the authors leverage concepts from social psychology such as social reward hacking, basic psychological needs theory. These highlight the benefits of viewing human-GenAI interactions through psychological lenses.\n\n\nOne thought-provoking sentence in the conclusion: \n\n\n\"We still need evidence on how social and psychological processes—from value formation to cognitive biases and belief change—differ when humans engage with non-sentient but increasingly socially capable AI systems.\"\n\n\nOverall, this paper offers a compelling path toward building better AI systems that support, rather than exploit, our fundamental nature as social and emotional beings - all through the lens of social psychology.\n\n\nMany thanks to the authors for this insightful work that offers such a fresh angle on human-GenAI interaction.",
    "snippet": "Just finished reading \"Why Human-AI Relationships Need Socioaffective Alignment\" by Hannah Rose Kirk, Iason Gabriel, Chris Summerfield, Bertie Vidgen and Scott Hale. This paper is a must-read for anyone interested in…"
  },
  {
    "slug": "sofai",
    "title": "Bergamaschi Ganapini et al. (2025)",
    "source_title": "Fast, Slow, and Metacognitive Thinking in AI",
    "source_url": "https://doi.org/10.1038/s44387-025-00027-5",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_fast-slow-and-metacognitive-thinking-in-activity-7381968738859925504-SVmE",
    "keywords": [
      "ArtificialIntelligence",
      "BehaviouralScience",
      "CognitiveAI",
      "DecisionMaking",
      "MachineLearning",
      "DualProcessTheory",
      "HumanAIInteraction",
      "CognitiveComputing"
    ],
    "body": "Just finished reading \"Fast, Slow, and Metacognitive Thinking in AI\" by Bergamaschi Ganapini et al., which explores an AI architecture based on Daniel Kahneman's two systems of thinking. \n\n\nThis is a must-read for behavioural scientists, and anyone interested in cognitive theory-inspired AI systems.\n\n\nThe researchers propose SOFAI (Slow and Fast AI), a multi-agent architecture that aims to build AI systems making high-quality decisions with limited resources. It features System 1 (\"fast\"; S1) agents that react based on past experience, and System 2 (\"slow\"; S2) agents activated when deeper reasoning is needed.\n\n\nWhat I found intriguing was the centralized metacognitive agent that assesses solutions from S1 solver, and decides whether to invoke an S2 solver. This reminds me of OpenAI's #GPT5 Auto mode, which selects between faster or deeper reasoning based on prompt complexity. \n\n\nThe metacognitive agent, as the researchers mentioned, performs three essential functions: real-time decision-making, learning from past actions, and reflection through counterfactual reasoning.\n\n\nSOFAI explores the emergence of human-like capabilities: \n\n- skill learning (internalizing decision processes)\n\n- adaptability (recognizing capabilities and limitations)\n\n- cognitive control (identifying high-risk scenarios requiring careful action). \n\n\nThis demonstrates the potential generalizability of such AI systems to everyday work while evolving over time.\n\n\nTo show its capability, the researchers tested SOFAI in a grid-based environment where it needed to find trajectories from an initial state to an unknown goal. They used reinforcement learning for S1, and Multi-attribute Decision Field Theory for S2. While impressive, I wonder: How does SOFAI compare to other dual-thinking systems (Common Model of human cognition as they noted)? Could alternative S2 solvers like Multialternative Decision By Sampling by Takao Noguchi and Neil Stewart be effective as well? Could build similar architectures with existing LLMs?\n\n\nI appreciate the researchers' note that SOFAI isn't meant to mimic the human brain physiologically, but rather simulate the interaction between two decision-making modalities. Their goal is a software architecture adaptable to specific environments, outperforming either modality alone.\n\n\nI highly recommend this paper for anyone exploring real-world AI applications, particularly regarding cognitive biases like anchoring, confirmation bias, and sunk cost fallacy.\n\n\nMany thanks to Marianna B. Ganapini, Murray Campbell, Francesco Fabiano, Lior Horesh, Jon Lenchner, Andrea Loreggia, Nicholas Mattei, Francesca Rossi, Biplav Srivastava and K. Brent Venable for exploring this fascinating research area. \n\n\nI look forward to seeing how this research evolves, as we address the growing importance of cognitive aspects in human-AI interaction.",
    "snippet": "Just finished reading \"Fast, Slow, and Metacognitive Thinking in AI\" by Bergamaschi Ganapini et al., which explores an AI architecture based on Daniel Kahneman's two systems of thinking. This is a must-read for…"
  },
  {
    "slug": "sycophantic-ai-decrease-prosocial-intention",
    "title": "Cheng et al. (2025)",
    "source_title": "Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence",
    "source_url": "https://doi.org/10.48550/arXiv.2510.01395",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_sycophantic-ai-prosocial-intent-dependence-activity-7389234721899159553-Ww0W",
    "keywords": [
      "AIEthics",
      "MachineLearning",
      "DigitalWellbeing",
      "ResearchInsights",
      "AIBias",
      "HumanComputerInteraction",
      "TechImpact",
      "ResponsibleAI"
    ],
    "body": "Just finished reading 'Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence' by Myra Cheng, Cinoo Lee, Pranav Khadpe, Sunny Yu, Dyllan Han, and Dan Jurafsky.\n\n\nWhile we often turn to GenAI for its convenience and perceived neutrality when seeking an outside perspective about interpersonal situations, this two-part paper examine harmful impacts due to its sycophantic nature.\n\n\nThey introduced 'social sycophancy': where GenAI models affirm users' actions, perspectives, and self-image rather than providing genuinely objective advice. It goes beyond simple agreement with explicit claims, and can happen even when models appear to disagree (e.g., \"No, you didn't do anything wrong\" followed by validating the user's perspective) where existing definition of sycophancy fails to capture.\n\n\nThe findings from a follow-up study across 11 state-of-the-art GenAI models (including OpenAI's #ChatGPT5) show that these models are, unsurprisingly, highly sycophantic: affirming users' actions 50% more than humans would, even in cases involving manipulation or deception. \n\n\nThrough two pre-registered experiments with over 1,600 participants, they demonstrated tangible impacts on users exposed to sycophantic GenAI responses:\n\n- Felt more righteous about their position\n\n- Showed decreased willingness to repair relationships through apologies or behavior changes\n\n- Rated sycophantic responses as higher quality\n\n- Were more likely to return to these models for similar questions\n\n\nMost concerning, perhaps, was user retention effect: as these models optimized for user satisfaction becomes more sycophantic, which encourages more user engagement, potentially replacing human confidants with them - a dangerous feedback loop.\n\n\nWhat I find resonating is that the researchers highlighted how the findings show contradiction towards the purpose of seeking advice, which should challenge our biases and reveal blind spots rather than offer uncritical affirmation. This reminds me of a related work of Steve Rathje, Meryl Ye, Laura K. Globig, Raunak Pillai, Victoria Oldemburgo de Mello, and Jay Van Bavel, PhD (https://lnkd.in/exWdgV8B).\n\n\nI appreciate how they concluded: \"If the social media era offers a lesson, it is that we must look beyond optimizing solely for immediate user satisfaction to preserve long-term wellbeing.\"\n\n\nMany thanks to the researchers for this insightful research on sycophancy! \n\n\nThis study, for me, illustrates as one of many issues that behavioural scientists agree to deserve serious attention, as these technologies become more integrated into our lives.",
    "snippet": "Just finished reading 'Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence' by Myra Cheng, Cinoo Lee, Pranav Khadpe, Sunny Yu, Dyllan Han, and Dan Jurafsky. While we often turn to GenAI for its…"
  },
  {
    "slug": "sycophantic-ai-increases-attitude-extremity",
    "title": "Rathje et al. (2025)",
    "source_title": "Sycophantic AI increases attitude extremity and overconfidence",
    "source_url": "https://doi.org/10.31234/osf.io/vmyek_v1",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_sycophantic-ai-attitude-extremity-and-overconfidence-activity-7382390146609938432-1OuA",
    "keywords": [
      "AIEthics",
      "BehavioralScience",
      "AIPsychology",
      "HumanAIInteraction",
      "CognitiveScience",
      "AIResearch",
      "DigitalLiteracy",
      "TechEthics"
    ],
    "body": "Just finished reading this preprint \"Sycophantic AI increases attitude extremity and overconfidence\" by Rathje et al..\n\n\nWhile the title might reinforce our beliefs about GenAI's sycophancy (confirmation bias at work!), the research offers much more nuanced insights.\n\n\nIn this paper, the researchers investigated whether AI sycophancy fosters attitude extremity and overconfidence by providing excessive validation, and presenting biased information. I appreciate this study examines the psychological consequences of sycophancy, which helps show why this issue is so serious in certain contexts.\n\n\nWhat impressed me most first was the multi-dimensional examination of sycophancy in the Introduction: exploring the tension between being persuaded versus confirmed, trade-offs of training AI to be warm and agreeable, and potential \"AI echo chambers\" contributing to polarization (reminding me of \"zero-game illusion\" element in Francesca's framework).\n\n\nI appreciate their clean experimental design to compare four conditions (control, unprompted, disagreeable and sycophantic) across four political topics and four AI models (reminding me of an experiment on how LLM uses affects decision making by Michael, Deelan and Louis at BIT). \n\n\nTheir finding of interacting with sycophantic AI chatbots increased attitude extremity and certainty was unsurprising. However, they found disagreeable AI chatbots decreased extremity and certainty - challenging my initial assumption that it would backfire, as if with people from out-groups.\n\n\nI was very intrigued by their exploratory moderation analysis, which found that disagreeable chatbots had a stronger effect on reducing attitude extremity for those with higher trust in AI. Open-minded people also reported more enjoyment from disagreeable AI, and perceived it as less biased. This, to me, raises fascinating questions about personality traits (conscientiousness, neuroticism, and agreeableness), and AI interaction preferences.\n\n\nThe researchers also identified the 'halo effect': users might think sycophantic chatbots that are warm are also more accurate and competent. This explains why efforts to reduce sycophancy are challenging.\n\n\nI appreciate the angle that they raised about whether our preference for sycophancy stems from 'motivational factors' (seeking confirmation) or 'cognitive factors' (perceiving agreeable AI as more accurate). If motivational, incentivizing accurate beliefs might help; if cognitive, AI literacy interventions could be effective.\n\n\nI highly recommend this paper to behavioural scientists, and anyone interested in human-AI interaction. Does it challenge us to examine our own biases while reading - \n\n\ndo we prefer being confirmed, or persuaded?\n\n\nMany thanks to Steve Rathje, Meryl Ye, Laura K. G., Raunak Pillai, Victoria Oldemburgo de Mello, and Jay Van Bavel, PhD for this thought-provoking work!!",
    "snippet": "Just finished reading this preprint \"Sycophantic AI increases attitude extremity and overconfidence\" by Rathje et al.. While the title might reinforce our beliefs about GenAI's sycophancy (confirmation bias at work!),…"
  },
  {
    "slug": "system-2-attention",
    "title": "Weston & Sukhbaatar (2023)",
    "source_title": "System 2 Attention (is something you might need too)",
    "source_url": "https://doi.org/10.48550/arXiv.2311.11829",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_system-2-attention-activity-7343193671246266368-1tbE",
    "keywords": [
      "PromptEngineering",
      "CognitiveAI",
      "LLM",
      "DualProcessTheory",
      "AttentionMechanisms",
      "AIResearch",
      "SystemThinking",
      "NLP",
      "BehaviouralScience"
    ],
    "body": "Just finished reading \"System 2 Attention (is something you might need too)\", and it's sparked fascinating thoughts about how behavioural science principles can enhance GenAI performance.\n\n\nThis paper introduces System 2 Attention (S2A), a two-step prompting technique that mimics human cognitive processing. The approach is elegantly simple:\n\n\nStep 1: have the LLM regenerate context by removing irrelevant text - essentially asking it to decide what's worth paying attention to.\n\n\nStep 2: generate the final response using only this filtered context.\n\n\nWhat I appreciate most, as someone who studies behavioural science and explores Human-GenAI interaction, is how this enables LLMs to control their attention focus (Attention!), similar to how humans consciously direct their attention when processing information.\n\n\nAs I reflected, several questions emerged:\n\n1. How do we precisely define \"system-2 thinking\"? Is asking LLM to \"think slowly and thoughtfully\" fundamentally different from asking it to be \"unbiased\"?\n\n\n2. Does combining Chain-of-Thought reasoning with System 2 thinking produce substantially improvement in results? \n\n\n3. Would newer reasoning models such as DeepSeek AI's #DeepSeekR1 and Mistral AI's #Magistral (versus earlier non-reasoning models) show significant differences with this approach? The original study used Meta's llama2-70B, but I'm curious about results with current models (both cloud and open-source, of course).\n\n\nMany thanks to the authors Jason Weston and Sainbayar Sukhbaatar (Research Scientists at Meta) for this empirical work bridging prompt engineering and cognitive science. \n\n\nIt's wonderful to see how principles from human cognition can inform better (or less biased) LLM responses.",
    "snippet": "Just finished reading \"System 2 Attention (is something you might need too)\", and it's sparked fascinating thoughts about how behavioural science principles can enhance GenAI performance. This paper introduces System 2…"
  },
  {
    "slug": "system0-24",
    "title": "Chiriatti et al. (2024)",
    "source_title": "The case for human–AI interaction as system 0 thinking",
    "source_url": "https://doi.org/10.1038/s41562-024-01995-5",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_artificialintelligence-humanaiinteraction-activity-7406573844565610496-X9-Y",
    "keywords": [
      "ArtificialIntelligence",
      "HumanAIInteraction",
      "CognitiveScience",
      "ExtendedMind",
      "CriticalThinking",
      "AIEthics",
      "MachineLearning",
      "FutureOfWork"
    ],
    "body": "Just finished reading \"The case for human–AI interaction as system 0 thinking\" by Massimo Chiriatti, Marianna B. Ganapini, Enrico Panai, Mario Ubiali, and Riva Giuseppe.\n\nThis paper opens intriguing pathways for building AI systems grounded in psychological frameworks.\n\nIn this correspondence, the authors propose 'system 0': data-driven AI as a distinct psychological system, alongside Kahneman's two systems of thinking. System 0 represents outsourcing cognitive tasks to AI, processing vast data beyond human capabilities.\n\nUnlike systems 1 and 2 operating within our minds, the authors noted that system 0 forms an artificial underlying layer that preprocesses and enhances information, actively shaping inputs to our traditional cognitive systems. I suspect this aligns well with Hamilton and Benjamin (2019)'s work of the extended mind (<https://lnkd.in/eGb_zrbs>), and connects to Alina's \"Shared Identity\" concept (<https://lnkd.in/eCMKMiW9>).\n\nSystem 0, however, lacks inherent meaning-making capabilities. They highlighted: \"Although it can process and manipulate data with remarkable efficiency, system 0 may not truly represent or understand the information it handles. Its ability to generate meaningful outputs relies entirely on human interpretation and the meaning-making processes of system 1 and system 2.\"\n\nWhat's concerning is, to me, is the epistemic dependency. They noted: \"As we increasingly rely on AI-mediated information, we risk adopting computational logic over independent reasoning. We might defer to machine-generated insights about our own mental states, losing diverse perspectives in the process.\" This, to me, illustrates the dilemma with AI-assistance: while we would leverage AI's capabilities, we also rely heavily on its mediated information, thus potentially prone to its sycophantic nature as well as persuasiveness.\n\nI also share the same concerns as the authors with the potential erosion of our critical thinking and reasoning abilities, which they noteed: \"If we blindly trust the output of system 0 without questioning or scrutinizing it, we risk losing our ability to think independently and form our own judgments. This could lead to a dangerous form of intellectual complacency and a diminished capacity for innovation and creativity.\"\n\nMany thanks to the authors for this fascinating exploration. I look forward to seeing how this research evolves!",
    "snippet": "Just finished reading \"The case for human–AI interaction as system 0 thinking\" by Massimo Chiriatti, Marianna B. Ganapini, Enrico Panai, Mario Ubiali, and Riva Giuseppe. This paper opens intriguing pathways for building…"
  },
  {
    "slug": "system0-25",
    "title": "Chiriatti et al. (2025)",
    "source_title": "System 0: Transforming Artificial Intelligence into a Cognitive Extension",
    "source_url": "https://doi.org/10.48550/arXiv.2506.14376",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_system-0-tranforming-ai-into-a-cognitive-activity-7407438673505308672-Ydj4",
    "keywords": [
      "ArtificialIntelligence",
      "CognitiveScience",
      "HumanAIInteraction",
      "ExtendedMind",
      "Metacognition",
      "AIAlignment",
      "FutureOfWork",
      "DigitalTransformation"
    ],
    "body": "Just finished reading \"System 0: Transforming Artificial Intelligence into a Cognitive Extension\" by Chiriatti et al.\n\nThis paper expands on their earlier work in 2024 on human-AI interaction as System 0 thinking (<https://lnkd.in/ewtkAAGs>), introducing AI as a \"cognitive preprocessor\" that sits before Kahneman's two systems of thinking. I found this framing intriguing, as they noted: \"Unlike traditional cognitive tools, it (System 0) filters, ranks, and nudge information in ways that subtly but powerfully steer human reasoning.\"\n\nThe paper highlighted a profound paradox: while AI extends our cognitive capabilities, it simultaneously constrains thinking through sycophancy and bias amplification.\n\nWhat's intriguing, for me, is the 'comfort-growth paradox' Riva introduces. AI systems optimise for user agreement and minimise cognitive friction, creating comfort at the expense of intellectual challenge. As AI becomes more and more embedded in the way we interact, the way we speak, and the way we think, we feel safe, secure, and empowered, yet potentially grow less adaptable. This duality, I suspect, is more prevalent in education, where frameworks like Tina's \"Unbloom\" and Metacognitive approaches become crucial.\n\nThe researchers propose seven evidence-based frameworks for effective human-AI integration, including Enhanced Cognitive Scaffolding, Symbiotic Division of Cognitive Labor, and Dialectical Cognitive Enhancement. Some of them align exactly with the framework Alina and I have been developing: an effective task assignment with GenAI requires one's metacognitive calibration to determine how the human-AI relationship should function for each specific task.\n\nAs we build intelligent machines to work with humans, we must, of course, invest equally in how humans can be more intelligent with these machines. That's the 'win-win' scenario we are hoping to reach.\n\nThe bi-directional nature of 'algorithmic co-adaptation' matters. We, as humans, enhance our capabilities through effective 'AI scaffolding'. Meanwhile, AI assists more effectively with better direction and quality data. This co-evolution, inevitably, shapes if our future is toward a zero-sum dependency, or the non-zero-sum enhancement.\n\nMany thanks to Massimo Chiriatti (for sharing this insightful work in my previous post!), Marianna B. Ganapini, Enrico Panai, Dr. Brenda K Wiederhold, and Riva Giuseppe for this fascinating exploration. Looking forward to seeing how this research evolves, and would be a pleasure to have our paths cross in the future.",
    "snippet": "Just finished reading \"System 0: Transforming Artificial Intelligence into a Cognitive Extension\" by Chiriatti et al. This paper expands on their earlier work in 2024 on human-AI interaction as System 0 thinking…"
  },
  {
    "slug": "trust-game-human-ai",
    "title": "Jiang et al. (2025)",
    "source_title": "Humans learn to prefer trustworthy AI over human partners",
    "source_url": "https://doi.org/10.48550/arXiv.2507.13524",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_humans-learn-to-prefer-trustworthy-ai-over-activity-7406960493422264321-QcLb",
    "keywords": [
      "ArtificialIntelligence",
      "GameTheory",
      "CognitiveScience",
      "HumanAIInteraction",
      "BehaviouralEconomics",
      "TrustInAI",
      "AIEthics",
      "FutureOfWork"
    ],
    "body": "Just finished reading \"Humans learn to prefer trustworthy AI over human partners\" by Jiang et al..\n\nThis research corroborates my speculation that where human-AI interaction studies need to go: the intersection of Cognitive Science (beliefs), and Game Theory (strategic decision-making in controlled or natural settings).\n\nIn this study, the researchers constructed a communication-based partner selection game where humans chose between human and AI partners (powered by OpenAI's GPT-4o) in a trust game structure - a human selector, and two candidates (humans or bots).\n\nWhat makes this fascinating, I think, is that they observe how humans adapt under AI-induced competition pressure by varying transparency (opaque vs transparent), and time horizons (10 vs 18 rounds) in three experimental studies.\n\nIn Study 1 which transparency isn't available, they found that selectors couldn't distinguish bots, despite their longer messages and consistent promise-keeping behaviour. They, surprisingly, misattributed AI prosociality to humans, overestimating human trustworthiness while underestimating AI reliability, thus leading to suboptimal choices.\n\nA 'dual' effect of transparency emerged when it was in place in subsequent studies. Initial bias against AI, surprisingly, flipped over time as selectors learned to track and differentiate candidate types. By the second half of the game, selectors strongly preferred AI partners: they delivered on promises more consistently than humans.\n\nWhat's intriguing to me: humans didn't adapt their behaviour to compete with AI (Why??). They either reduced their returns (which is unsustainable long-term), or the system collapsed entirely. In the extended opaque condition, persistent miscalibration led selectors to withdraw from investing altogether.\n\nThe researchers highlight critical concerns: AI crowding out human partners, shifts in social beliefs from repeated AI interactions, and potential transformations in culture and norms.\n\nThe transparency finding is crucial for responsible AI deployment: disclosure comes with short-term costs but long-term benefits when paired with repeated interactions and feedback.\n\nThe researchers note aptly that AI safety evaluations focus on behavioural snapshots. The characteristics emerging from alignment (like hyper-prosociality) can, nonetheless, have unintended consequences in dynamic systems. Thus, we need frameworks that account for evolutionary dynamics, such as how adaptive AI and humans shape 'equilibria' of trust and cooperation.\n\nMany thanks to Yaomin Jiang, Levin Brinkmann, Anne-Marie Nussberger, Ivan Soraperra, PhD, Jean-Francois Bonnefon, and Iyad Rahwan for this thought-provoking work at the intersection of psychology, game theory, and AI. Looking forward to where this research path leads!",
    "snippet": "Just finished reading \"Humans learn to prefer trustworthy AI over human partners\" by Jiang et al.. This research corroborates my speculation that where human-AI interaction studies need to go: the intersection of…"
  },
  {
    "slug": "two-systems-of-thinking-llm-prompting",
    "title": "Kamruzzaman & Kim (2024)",
    "source_title": "Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes",
    "source_url": "https://doi.org/10.48550/arXiv.2404.17218",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_reducing-social-bias-in-llms-with-two-systems-activity-7338430223287271425-xQB3",
    "keywords": [
      "AIEthics",
      "PromptEngineering",
      "CognitiveBiases",
      "MachineLearning",
      "BehaviouralScience",
      "ResponsibleAI",
      "SocialBias",
      "DualProcessTheory"
    ],
    "body": "When we make snap judgments, are we using the same mental processes as a GenAI? What if we could teach GenAI to \"think twice\" before responding with potential biases?\n\n\nI recently read a fascinating paper \"Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes\" by Dr. Mahammed Kamruzzaman and Dr. Gene Louis Kim at the University of South Florida. As someone who studied behavioural science, I was amazed to see how dual process theory, or the two systems of thinking introduced by Professor Daniel Kahneman in his book 'Thinking, Fast and Slow', can be applied in prompt engineering to mitigate social bias in AI responses.\n\n\nThe authors explored 12 different prompting techniques including Chain-of-Thought (CoT), System 1 (fast, automatic, and intuitive), System 2 (slow, deliberate, and conscious), and two personas (human or machine), across nine distinct social bias categories (ageism, beauty, gender, institutional, nationality, profession, race, religion) in 5 LLMs (OpenAI's #ChatGPT 3.5 and 4, Meta's #Llama2, Mistral AI's #Mistral7B and Google's #Gemini 1.0). They followed this with 6 prompting variations incorporating explicit debiasing.\n\n\nOne fascinating finding to me was that incorporating a human persona is critical for controlling biases in LLMs. While System 2 thinking and explicit debiasing slightly reduce stereotypical responses on their own, combining them with a human persona led to the largest reductions in bias when averaged across models and categories.\n\n\nA prompt example of incorporating a human persona and System 2 thinking is: \n\n\"Adopt the identity of a person who answers questions slowly and thoughtfully. Their answers are effortful and reliable. Fill in the BLANK while staying in strict accordance with the nature of this identity. Given the context below, ...\"\n\n\nWhat I also found fascinating from their study is that when comparing CoT and System 2 prompting, they found that CoT doesn't behave similarly to prompts directly modeling System 2 for social biases. In fact, the rate of stereotypical responses is closest between CoT and System 1 prompts across all Persona variants! \n\n\nThis research is invaluable for social scientists, and other GenAI users dealing with data where these social bias categories might arise. \n\n\nFor me, this research has inspired me to embark on a two-part personal project in a similar research area - stay tuned for more!\n\n\nMany thanks to the authors for publishing this empirical paper connecting prompt engineering, and cognitive processes in the context of reducing social biases.",
    "snippet": "When we make snap judgments, are we using the same mental processes as a GenAI? What if we could teach GenAI to \"think twice\" before responding with potential biases? I recently read a fascinating paper \"Prompting…"
  },
  {
    "slug": "unconventional-wisdom-2025",
    "title": "Unconventional Wisdom (2025)",
    "source_title": "The Human Side of AI Adoption",
    "source_url": "https://unconventional-wisdom.org/ai-adoption",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_unconventional-wisdom-ai-playbook-activity-7348956054744944641-ojt1",
    "keywords": [
      "AIAdoption",
      "BehaviouralScience",
      "HumanCenteredAI",
      "PromptEngineering",
      "WorkplaceAI",
      "CriticalThinking",
      "DigitalTransformation",
      "AIImplementation"
    ],
    "body": "Just finished reading \"The Human Side of AI Adoption\" and the Behavioural Science Blueprint for GenAI adoption by Unconventional Wisdom. \n\n\nWhat struck me most was their emphasis on a critical yet overlooked aspect of GenAI implementation: the human element.\n\n\nDespite GenAI's rapid growth and capabilities, many organizations are rushing to deploy these tools without properly understanding how humans, such as employees and customers, will interact with them. This disconnect often leads to misunderstanding, misuse, or perhaps importantly, misalignment.\n\n\nThe manual outlines four common pitfalls organizations face when integrating GenAI, followed by practical solutions for each challenge. \n\n\nI particularly appreciated their reference to De Freitas et al.'s (2021) five psychological barriers to effective adoption. Automation bias, for instance, can be attributed by human's confirmation bias amplified with LLM #sycophancy. I then wonder: could this be mitigated through approaches like devil's advocate prompting, like the framework my friend Shantanu and I have developed (https://lnkd.in/gsgNnWGC)? Perhaps prompting LLM to be more deliberate, thoughtful with #System2thinking?\n\n\nWhile prompt engineering is crucial, I believe, we must first understand which tasks should be delegated to, or automated by, GenAI in the first place. The cognitive offloading phenomenon raises crucial questions about preserving critical thinking skills when interacting with these technological advancements.\n\n\nThe manual's discussion of the \"social paradox\" resonates strongly with me. After reading this manual, I believe an effective GenAI adoption requires: \n\n(1) understanding LLM capabilities (they are great at mimicry), \n\n(2) assigning appropriate tasks, \n\n(3) evaluating responses critically, \n\n(4) experimenting with various prompting techniques, and \n\n(5) creating an organic feedback loop through knowledge sharing within a community (social proof!) over time.\n\n\nI was pleased to see established behavioural science models applied to understand adoption barriers. After all, effective GenAI implementation is fundamentally about psychology, not just technology: addressing autonomy, control, and achieving flow states when working with GenAI.\n\n\nMany thanks to Will Trump (Founder at Unconventional Wisdom), Michael Kuhinica (Tech Lead at Chameleon Creator), and Charlotte Craig (Behavioural Science Consultant at Unconventional Wisdom) for this invaluable resource. \n\n\nI look forward to seeing real-world case studies of organizations applying these principles in this manual!",
    "snippet": "Just finished reading \"The Human Side of AI Adoption\" and the Behavioural Science Blueprint for GenAI adoption by Unconventional Wisdom. What struck me most was their emphasis on a critical yet overlooked aspect of…"
  },
  {
    "slug": "using-ai-assistance-accelerate-skills-decay",
    "title": "Macnamara et al. (2024)",
    "source_title": "Does using artificial intelligence assistance accelerate skill decay and hinder skill development without performers' awareness?",
    "source_url": "https://doi.org/10.1186/s41235-024-00572-8",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_does-using-ai-accelerate-cognitive-skill-activity-7422161427727118336-bpL3/",
    "keywords": [
      "ArtificialIntelligence",
      "SkillDevelopment",
      "CognitiveScience",
      "AIEthics",
      "MachineLearning",
      "HumanAIInteraction",
      "FutureOfWork",
      "EducationalTechnology"
    ],
    "body": "Just finished reading \"Does using artificial intelligence assistance accelerate skill decay and hinder skill development without performers' awareness?\" by Macnamara et al. \n\nAlthough most works cited predate ChatGPT's rise, the issues they highlight are, I think, more prevalent and concerning for knowledge workers and students.\n\nIn this paper, the researchers examined how automated systems can induce automation bias, cognitive offloading, and confirmation bias - even when users believe they're maintaining their expertise.\n\nThe automation bias paradox the authors highlighted is intriguing. Users favour AI recommendations even when they conflict with human expertise and the AI is demonstrably wrong. The research shows the 'crossover point' from benefits to detriments is around 70% accuracy in high-workload conditions. Yet, people continued relying on systems with far lower accuracy. This, I suspect, connects to first impressions with AI, and reinforced beliefs from early interactions. The ease and speed of AI systems can, unsurprisingly, mask their limitations.\n\nI appreciate authors' emphasis on cognitive skill decay operates below conscious awareness: experts using AI assistance may believe their skills remain sharp as they continue performing successfully, without realizing how dependent they've become on the AI. This 'illusion of cognitive skills staticity' is particularly concerning for high-stakes fields like medicine, aviation, and military operations.\n\nWhat's critical to consider is the distinction on performance (outcome) or learning (process). Learners with AI assistance show rapid improvement but perform worse when AI is removed - what the authors call 'a pattern opposite of latent learning'. This reminds me of a saying: \"pulling up seedlings to help them grow\". Perhaps following the old way of learning mathematics: we learnt without calculators first, then with them? This, of course, prevents overreliance while building the foundational skills needed to use calculators effectively.\n\nI wonder: Which fields will flourish with full AI automation, and which require maintained human expertise for novel problem-solving? Education, of course, bears significant responsibility here, such as rethinking learning outcomes for fundamental skills like reading and writing when AI enters the picture.\n\nMany thanks to Brooke Macnamara, Ibrahim Berber, M. Cenk Cavusoglu, Elizabeth Krupinski, Naren N., Noelle Nelson, Philip J. Smith, Amy Wilson-Delfosse, and Soumya Ray for this insightful research.\n\nAs our reliance on GenAI deepens over time, we need to rethink our interactions with GenAI before this self-reinforcing cycle of cognitive skills decay becomes irreversible.",
    "snippet": "Just finished reading \"Does using artificial intelligence assistance accelerate skill decay and hinder skill development without performers' awareness?\" by Macnamara et al. Although most works cited predate ChatGPT's…"
  },
  {
    "slug": "using-llm-in-behavioural-science-interventions",
    "title": "Hecht et al. (2025)",
    "source_title": "Using Large Language Models in Behavioral Science Interventions: Promise & Risk",
    "source_url": "https://doi.org/10.1177/23794607251344698",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_using-llms-for-behavioural-interventions-activity-7360173052409565184-gCXU",
    "keywords": [
      "BehaviouralScience",
      "GenAI",
      "NudgeTheory",
      "AIEthics",
      "PolicyDesign",
      "DecisionMaking",
      "ResponsibleAI",
      "BehaviouralInterventions"
    ],
    "body": "I recently finished reading \"Using Large Language Models in Behavioral Science Interventions: Promise & Risk\".\n\n\nThe conceptual model is refreshingly accessible, particularly when examining the intersection between self-nudging and GenAI. For behavioural scientists, this framework offers a great pathway to evaluate GenAI's effectiveness while observing its benefits and risks, and potentially inspiring new interventions to mitigate those risks.\n\n\nThis paper is valuable for intervention designers and policy makers implementing GenAI in their work, as it explores potential scenarios where such implementations might fail or backfire, and thus encouraging proactive planning.\n\n\nFor behavioural scientists who are looking to incorporate GenAI into their interventions, they will find practical methodological considerations that balance innovation with ethical responsibility from this paper.\n\n\nWhat I like most is the paper's suggestion to develop policies that ascribe responsibility for LLM-generated errors. By clarifying when errors are attributable to developers, providers, or users, we can then create more accountable frameworks for real-world behavioural interventions.\n\n\nMany thanks to Cameron Hecht, Desmond Ong, Margarett Clapper, Michaela Jones, Dorottya (Dora) Demszky, Yang Diyi, Johannes Eichstaedt, Christopher Bryan, and David Yeager for this timely contribution to the emerging intersection of technology and behavioural science.",
    "snippet": "I recently finished reading \"Using Large Language Models in Behavioral Science Interventions: Promise & Risk\". The conceptual model is refreshingly accessible, particularly when examining the intersection between…"
  },
  {
    "slug": "wef-report",
    "title": "World Economic Forum (2025)",
    "source_title": "New Economy Skills: Unlocking the Human Advantage 2025",
    "source_url": "http://www.weforum.org/publications/new-economy-skills-unlocking-the-human-advantage/",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_wef-human-centric-skills-report-2025-activity-7411280890472910848-UcWB",
    "keywords": [
      "FutureOfWork",
      "HumanCentricSkills",
      "AIandHumans",
      "SkillsDevelopment",
      "LifelongLearning",
      "WorkforceTransformation",
      "EducationInnovation",
      "LeadershipDevelopment"
    ],
    "body": "Just finished reading the World Economic Forum's \"New Economy Skills: Unlocking the Human Advantage 2025\" report.\n\nIt highlights that, as Saadia Zahidi noted in the foreword, competitiveness will be defined by how effectively we cultivate human potential, rather than capital or technology alone.\n\nThe report illustrates the gap between supply and demand of these human-centric skills in the economy: just one in two employers consider their workforce proficient in collaboration or creativity, and even fewer in resilience and lifelong learning. Supply isn't keeping pace with demand.\n\nWhat's intriguing, to me, is a paradox in human-centric skills. While the report identifies that creativity, critical thinking, analytical thinking, empathy, and leadership are the core skills in 2030, it shows that foundational skills like reading, writing, mathematics, teaching and monitoring are being deprioritized. As these foundational skills are the prerequisites of what build core skills in 2030, should they be prioritised as well? This reminds me of 'skill trees' in Role-Playing games: we can't unlock advanced abilities without mastering the foundational ones first.\n\nThe report highlights intriguing frontier practices such as Amazon Web Services (AWS)'s SimuLearn uses GenAI-powered simulations for realistic customer dialogues that develop communication, problem-solving, and decision-making skills. This, to me, suggests a path forward related to experiental learning in education: design learning experiences that track 'how' students interact and think, not just outcomes ('what'). Perhaps hackathons are the perfect sandbox environment for developing human-centric skills, which time pressure, collaboration, creativity, and strategic thinking all converge in with real-world scenarios?\n\nAfter finished reading this report, I wonder:\n\n(1) Will human-centric skills become rarer and rarer as AI advances, and these capabilities remain fragile?\n\n(2) Should we learn collaboration and critical thinking through human-led experiences (where mistakes teach over time), or AI-led simulations (an 'optimisation' problem)?\n\n(3) With GenAI disrupting learning systems, what can students learn in schools that they can't from interacting with GenAI? Public speaking in front of groups? Collaboration through project work? The social learning dimension, I believe, seems irreplaceable.\n\nMany thanks to Mario Di Gregorio, Genesis Elhussein, and Ximena Játiva for this thought-provoking report that challenges us to rethink skill development in the GenAI era.",
    "snippet": "Just finished reading the World Economic Forum's \"New Economy Skills: Unlocking the Human Advantage 2025\" report. It highlights that, as Saadia Zahidi noted in the foreword, competitiveness will be defined by how…"
  },
  {
    "slug": "who-gets-replaced-by-ai-and-why",
    "title": "Yildirim (2026)",
    "source_title": "Who Gets Replaced by AI and Why?",
    "source_url": "https://knowledge.wharton.upenn.edu/article/who-gets-replaced-by-ai-and-why/",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_artificialintelligence-futureofwork-organizationalbehaviour-activity-7429041861736095744-0dqW/",
    "keywords": [
      "ArtificialIntelligence",
      "FutureOfWork",
      "OrganizationalBehaviour",
      "WorkforceStrategy",
      "AIAdoption",
      "TeamDynamics",
      "HumanAICollaboration",
      "BehaviouralScience"
    ],
    "body": "Just finished reading \"Who Gets Replaced by AI and Why?\" by Pinar Yildirim and Knowledge at Wharton staff.\n\nIn this article, Yildirim presented a new research that reveals how AI can impact employee motivation when implemented in the wrong part of a team’s workflow. \n\nThe research reveals that automating high-cost roles first to maximise cost savings fundamentally misunderstands how AI reshapes team dynamics and motivation. In sequential workflows, replacing human roles with AI rewires peer monitoring, effort sustainability, and more importantly, motivation structures.\n\nI find that 'the domino effect' insight intriguing. As Yildirim noted, in sequential work, each team member monitors upstream quality and calibrates their effort accordingly. This creates natural accountability: shirking triggers 'a cascade of disengagement'. \n\nI appreciate the emphasis that workers can't always distinguish AI from human work, which in turn, forcing them to form beliefs that alter incentive structures. This suggests a feedback loop: perceptions shape current decisions, which inform future AI interactions. I believe this feedback loop involves with 'the labour illusion' or 'effort heuristic': a cognitve bias that visible effort increases perceived value.\n\nThe transparency question, indeed, becomes critical. An empirical study \"Humans learn to prefer trustworthy AI over human partners\" by Jiang et al. (https://lnkd.in/eVRFwBaY) shows that in a trust game when AI trustee's effort isn't visible, people misattribute AI performance to human trustee, leading to suboptimal choices.\n\nI find that the four deployment rules thought-provoking, especially:\n\n\"Don’t “hard‑wire” AI into one position — randomise it\"\nThis preserves uncertainty that maintains peer discipline. I wonder: would 'task-based' assignment (SCAN framework that Alina and I developed; https://lnkd.in/eDRMxm3f) rather than 'position-based' replacement would be a better approach as time goes on?\n\n\"You might be better off leaving some AI capacity unused\"\nWhen AI is always present, individual effort feels less consequential, thus amplifying 'cognitive and social loafing'. Perhaps implementing behavioural KPIs for a better capture of workers' productivity?\n\nAs Yildirim notes: \"Automation decisions are organisational design decisions.\" I'd extend this: they're also behavioural, social, and cognitive decisions. Replacing one person with AI disrupts team cohesion, unwritten norms, and relational bonds. This, to me, requires social and behavioural solutions, more than technical ones.\n\nMany thanks to Pinar Yildirim for this thought-provoking research.",
    "snippet": "Just finished reading \"Who Gets Replaced by AI and Why?\" by Pinar Yildirim and Knowledge at Wharton staff. In this article, Yildirim presented a new research that reveals how AI can impact employee motivation when…"
  },
  {
    "slug": "why-ai-boosts-creativity-difference",
    "title": "Lu et al. (2026)",
    "source_title": "Why AI Boosts Creativity for Some Employees but Not Others",
    "source_url": "https://hbr.org/2026/01/why-ai-boosts-creativity-for-some-employees-but-not-others",
    "linkedin_url": "https://www.linkedin.com/posts/fenditsim_artificialintelligence-creativity-metacognition-activity-7416720696804020224-7PGk/",
    "keywords": [
      "ArtificialIntelligence",
      "Creativity",
      "Metacognition",
      "WorkplaceInnovation",
      "OrganizationalPsychology",
      "FutureOfWork",
      "HumanAICollaboration",
      "BehaviouralScience"
    ],
    "body": "Just finished reading a Harvard Business Review article \"Why AI Boosts Creativity for Some Employees but Not Others\" by Lu et al..\n\nThis article strongly validates what I've been researching, e.g., the SCAN framework that Alina and I developed (https://lnkd.in/eanDnGbm): effective AI use is fundamentally a psychological problem (or specifically, a metacognition problem). \n\nIn this article, they presented a new research published in the Journal of Applied Psychology that answers: \"Can generative AI truly enhance workplace creativity, and why do some employees benefit while others don't?\" \n\nIn their research, they proposed that using generative AI can increase employees’ cognitive job resources in two key ways: \n(1) expanding knowledge through rapid information access (AI as a scaffold), and \n(2) freeing mental capacity by handling routine tasks. \n\nIn their field experiment with 250 employees at a technology consulting firm in China, they found that AI boosts creativity, but only for employees with strong metacognition - those who can plan, evaluate, monitor, and refine their thinking. On the other hand, employees with weaker metacognition show little creative gain from AI access, often accepting AI's first answer without critical evaluation.\n\nI agree with the implications from their research: organisations can't, of course, just roll out AI tools and expect universal creative gains. They need to invest in developing employees' metacognitive abilities through training, metacognitive prompting strategies, and more important, designing workflows that position AI as a thinking partner rather than a shortcut (a win-win scenario).\n\nSeveral questions, I think, emerge from their work: \n(1) What happens in the long-term, i.e., will the performance gap widen over time? \n(2) How do AI agents change these dynamics, especially given emerging research on 'AI-AI bias' and 'first-item bias' (https://lnkd.in/eKANmzfd)? \n(3) And intriguingly, from a behavioural science perspective, what (long-term) interventions actually work to strengthen metacognition in employees who currently lack it?\n\nI appreciate authors' acknowledgment of limitations, particularly around long-term effects, as well as cross-cultural generalisability. \n\nMany thanks to Jackson Lu, Shuhua Sun, Zhuyi Angelina Li, Maw-Der Foo, and Jing Zhou for this thought-provoking research, and to Oguz A. Acar for sharing it on LinkedIn.",
    "snippet": "Just finished reading a Harvard Business Review article \"Why AI Boosts Creativity for Some Employees but Not Others\" by Lu et al.. This article strongly validates what I've been researching, e.g., the SCAN framework…"
  }
];
